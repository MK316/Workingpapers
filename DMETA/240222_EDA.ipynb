{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPS5pZ2a4kmyfHDT230keKD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MK316/Workingpapers/blob/main/DMETA/240222_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis (EDA) as of 24.02. 22\n",
        "\n",
        "[data](https://raw.githubusercontent.com/MK316/Workingpapers/main/DMETA/data/data1.csv)"
      ],
      "metadata": {
        "id": "9BIbly6iFHR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1. Data to read"
      ],
      "metadata": {
        "id": "REsFqSAvHs-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/MK316/Workingpapers/main/DMETA/data/data1.csv\""
      ],
      "metadata": {
        "id": "DIE0DI_UHFwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(url, encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "Spqdbqz_HJnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2. Preliminary data check"
      ],
      "metadata": {
        "id": "pQs348hAHdV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "__h8AcOZHW-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keyword preprocessing\n",
        "\n",
        "Keywords: Keywords1 + Keywords2"
      ],
      "metadata": {
        "id": "toJQGe3BIuqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "\n",
        "# Combine 'Keywords1' and 'Keywords2' into a single 'Keywords' column\n",
        "# Here we assume you want to separate keywords from both columns by a comma\n",
        "# We also handle NaN values to avoid 'nan' strings in the combined column\n",
        "df['Keywords'] = df[['Keywords1', 'Keywords2']].apply(lambda x: ', '.join(x.dropna()), axis=1)\n",
        "\n",
        "# Now, you can drop the original 'Keywords1' and 'Keywords2' columns if they are no longer needed\n",
        "df.drop(['Keywords1', 'Keywords2'], axis=1, inplace=True)\n",
        "\n",
        "# Save the updated DataFrame to a CSV file\n",
        "filename = 'data2.csv'\n",
        "df.to_csv(filename, index=False)\n",
        "\n",
        "# To ensure the file is saved, you can list the files in the current directory\n",
        "!ls\n",
        "\n",
        "# If you need to download the file to your local system from Colab, you can use:\n",
        "from google.colab import files\n",
        "files.download(filename)\n"
      ],
      "metadata": {
        "id": "iNXh-HuaJT6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"data2.csv\", encoding=\"utf-8\")"
      ],
      "metadata": {
        "id": "sPAEhZkgJgwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descriptive stats\n",
        "\n",
        "Year info (as string)"
      ],
      "metadata": {
        "id": "WgfYEUrHK81G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the 'Year' column to string\n",
        "df['Year'] = df['Year'].astype(str)\n",
        "\n",
        "# Get descriptive statistics for 'Year' as string\n",
        "# Since 'Year' is now a string, traditional numerical descriptive stats don't apply.\n",
        "# However, we can get counts, unique values, most common value, and frequency of the most common value.\n",
        "year_descriptive_stats = df['Year'].describe()\n",
        "\n",
        "print(year_descriptive_stats)\n"
      ],
      "metadata": {
        "id": "erGJ1grSK-c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "barplot by year"
      ],
      "metadata": {
        "id": "4gWPsR27LdaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'Year' is a column in your DataFrame df\n",
        "year_counts = df['Year'].value_counts().sort_index()\n",
        "\n",
        "# Convert year_counts Series to DataFrame\n",
        "year_counts_df = year_counts.reset_index()\n",
        "year_counts_df.columns = ['Year', 'Frequency']\n",
        "\n",
        "print(year_counts_df)\n"
      ],
      "metadata": {
        "id": "l87FUd6lLtz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'Year' is already converted to string and df is your DataFrame\n",
        "year_counts = df['Year'].value_counts().sort_index()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "year_counts.plot(kind='bar')\n",
        "plt.title('Frequency of Publications by Year')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "cXm52SwPLfDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With frequency\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'Year' is already converted to string and df is your DataFrame\n",
        "year_counts = df['Year'].value_counts().sort_index()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = year_counts.plot(kind='bar')\n",
        "plt.title('Frequency of Publications by Year')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Frequency')\n",
        "plt.ylim(0,60)\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Annotating the bar plot with the frequency of each bar\n",
        "for p in bars.patches:\n",
        "    bars.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                  ha = 'center', va = 'center', xytext = (0, 10), textcoords = 'offset points')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KoTMRplSMLKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3. Keyword Analysis for 'Digital'\n",
        "To analyze the frequency of 'digital', you can start by creating a new column in your DataFrame that flags the presence of the word 'digital' in the 'Abstract', 'Title', or 'Keywords' columns."
      ],
      "metadata": {
        "id": "Xdjck3K0HgNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercase all relevant columns to standardize the search\n",
        "df['Abstract'] = df['Abstract'].str.lower()\n",
        "df['Title'] = df['Title'].str.lower()\n",
        "df['Keywords'] = df['Keywords'].str.lower()\n",
        "\n",
        "# Flag rows that mention 'digital'\n",
        "df['mentions_digital'] = df['Abstract'].str.contains('digital') | df['Title'].str.contains('digital') | df['Keywords'].str.contains('digital')\n"
      ],
      "metadata": {
        "id": "22WFWIcHHilV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Frequency of 'Digital' Over Time\n",
        "Now, calculate the frequency of abstracts mentioning 'digital' each year."
      ],
      "metadata": {
        "id": "AM8XWINRHmw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "digital_mentions_per_year = df[df['mentions_digital']].groupby('Year').size()\n"
      ],
      "metadata": {
        "id": "Dxe8815NHvCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5. Step 5: Plotting the Trends\n",
        "To visualize how the mention of 'digital' has changed over time, you can plot the frequency per year."
      ],
      "metadata": {
        "id": "9CjrfAwrHw3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming digital_mentions_per_year is a Series or DataFrame column with year as index and counts as values\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = digital_mentions_per_year.plot(kind='bar')\n",
        "plt.title('Frequency of \"Digital\" Mentions Over Time')\n",
        "plt.ylim(0,60)\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number of Mentions')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--')\n",
        "\n",
        "# Annotate each bar with the frequency value\n",
        "for p in bars.patches:\n",
        "    bars.annotate(f'{int(p.get_height())}',\n",
        "                  (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                  ha='center', va='bottom',\n",
        "                  xytext=(0, 5),\n",
        "                  textcoords='offset points')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7gXvKqJOHzoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Co-occurrence Analysis (Optional)\n",
        "For a simple co-occurrence analysis, you might start with identifying the most common words in abstracts that mention 'digital'. This requires more advanced text processing and is not covered in depth here, but you can begin with a basic approach using Counter from the collections module."
      ],
      "metadata": {
        "id": "s2f-MPoLIkuu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Without stopwords removal\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Combine all abstracts mentioning 'digital' into one large text\n",
        "digital_abstracts_text = ' '.join(df[df['mentions_digital']]['Abstract'].tolist())\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = re.findall(r'\\w+', digital_abstracts_text)\n",
        "\n",
        "# Count the words, excluding 'digital'\n",
        "word_counts = Counter(words)\n",
        "del word_counts['digital']\n",
        "\n",
        "# Display the most common words co-occurring with 'digital'\n",
        "print(word_counts.most_common(10))\n"
      ],
      "metadata": {
        "id": "3J49FEEHIl2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop words are removed"
      ],
      "metadata": {
        "id": "kw_lc-7hKCDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add stopwords: abstract specific HF words"
      ],
      "metadata": {
        "id": "4YDSC7T7NNaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "# Download the list of stopwords from nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "CQS7Osg9NQ_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up stopwords, you might want to add 'digital' explicitly if it's not already in the list\n",
        "stop_words = set(stopwords.words('english')) | {'digital'}"
      ],
      "metadata": {
        "id": "mUP2trqPNi04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually add 'digital' and other specific stopwords\n",
        "additional_stopwords = {'findings', 'based', 'research','second','language','english',\"two\",\"results\"}\n",
        "stop_words.update(additional_stopwords)\n"
      ],
      "metadata": {
        "id": "MnQL5ycFNlhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Combine all abstracts mentioning 'digital' into one large text\n",
        "digital_abstracts_text = ' '.join(df[df['mentions_digital']]['Abstract'].tolist()).lower()\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = re.findall(r'\\w+', digital_abstracts_text)\n",
        "\n",
        "# Remove stopwords from the list of words\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "# Count the words, excluding stopwords and 'digital'\n",
        "word_counts = Counter(filtered_words)\n",
        "\n",
        "# Display the most common words co-occurring with 'digital'\n",
        "print(word_counts.most_common(20))\n"
      ],
      "metadata": {
        "id": "fBr7m7bpKDcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These steps will give you a preliminary analysis of how the term 'digital' has been used and its co-occurrence with other terms in your dataset. Depending on your findings, you might refine your analysis to focus on specific years, topics, or co-occurring terms."
      ],
      "metadata": {
        "id": "0gzdRY4sIrnZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatize and find co-occurrence\n",
        "\n",
        "DMC to 'digital multimodal composition'"
      ],
      "metadata": {
        "id": "BUf9FxNKOan0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "#Define a mapping of abbreviations to their full forms\n",
        "abbreviation_expansions = {\n",
        "    '\\\\bdmc\\\\b': 'digital_multimodal_composition',  # Use word boundary regex to match whole word only\n",
        "    '\\\\bwtc\\\\b': 'willing_to_communicate'\n",
        "    # Add more abbreviations and their expansions as needed\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Manually add specific stopwords including 'digital'\n",
        "additional_stopwords = {'l2','digital','findings', 'based', 'research', 'second', 'language', 'english', \"two\", \"results\"}\n",
        "\n",
        "# Extend the stop words list with custom words\n",
        "stop_words = set(stopwords.words('english')) | additional_stopwords\n",
        "\n",
        "# Combine all abstracts mentioning 'digital' into one large text\n",
        "digital_abstracts_text = ' '.join(df[df['mentions_digital']]['Abstract'].tolist())\n",
        "\n",
        "# Apply abbreviation expansion before lowering case to ensure accurate replacement\n",
        "for abbr, expansion in abbreviation_expansions.items():\n",
        "    digital_abstracts_text = re.sub(abbr, expansion, digital_abstracts_text, flags=re.IGNORECASE)\n",
        "\n",
        "# Lower the case after expansions are done\n",
        "digital_abstracts_text = digital_abstracts_text.lower()\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = re.findall(r'\\w+', digital_abstracts_text)\n",
        "\n",
        "# Lemmatize words and remove those that are in the stop words list\n",
        "filtered_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "\n",
        "# Count the words, now excluding extended stopwords and simplified words\n",
        "word_counts = Counter(filtered_words)\n",
        "\n",
        "# Display the most common words co-occurring with 'digital'\n",
        "print(word_counts.most_common(20))\n"
      ],
      "metadata": {
        "id": "0Qmj3BQZOdDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize co-occurrence"
      ],
      "metadata": {
        "id": "ooND1WwGKVq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming word_counts is your Counter object from the previous step\n",
        "most_common_words = word_counts.most_common(30)\n",
        "\n",
        "# Unpack the words and their frequencies for plotting\n",
        "words, frequencies = zip(*most_common_words)\n",
        "\n",
        "# Create a bar chart\n",
        "plt.figure(figsize=(16, 8))\n",
        "plt.bar(words, frequencies, color='skyblue')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Top 10 Most Common Words Co-occurring with \"Digital\"')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7kyZfhPnKYY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 2. Trend analysis"
      ],
      "metadata": {
        "id": "icilFv2TT5Xo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Keyword Frequency Over Time\n",
        "First, calculate the frequency of 'digital' and its co-occurring terms per year."
      ],
      "metadata": {
        "id": "ujeNFJsuUekc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "# Convert all 'Abstract' entries to strings to avoid AttributeError\n",
        "df['Abstract'] = df['Abstract'].astype(str).str.lower()\n",
        "\n",
        "# Initialize a dictionary to hold counts of 'digital' per year\n",
        "digital_frequency_per_year = defaultdict(int)\n",
        "\n",
        "# Tokenize and count\n",
        "for _, row in df.iterrows():\n",
        "    # Ensure we're only considering non-missing 'Abstract' values\n",
        "    if row['Abstract'] != 'nan':  # Check for the string representation of NaN\n",
        "        words = set(row['Abstract'].split())  # Using set to count each word once per abstract\n",
        "        if 'digital' in words:\n",
        "            digital_frequency_per_year[row['Year']] += 1\n",
        "\n",
        "# Convert the dictionary to a DataFrame for easier handling\n",
        "digital_freq_df = pd.DataFrame(list(digital_frequency_per_year.items()), columns=['Year', 'Frequency']).sort_values(by='Year')\n",
        "\n",
        "print(digital_freq_df)\n"
      ],
      "metadata": {
        "id": "Fr-O63bvUhM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Co-occurrence Matrix for 'Digital' and Other Terms\n",
        "Next, create a co-occurrence matrix. This matrix will show how often other words occur with 'digital' in the same abstracts across all years. This part is a bit more involved and requires careful consideration of the entire dataset."
      ],
      "metadata": {
        "id": "NaQypErZUleq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatized abstract = data3.csv"
      ],
      "metadata": {
        "id": "3kpefJzuheVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "url = \"https://raw.githubusercontent.com/MK316/Workingpapers/main/DMETA/data/data1.csv\"\n",
        "df = pd.read_csv(url, encoding=\"utf-8\")\n",
        "\n",
        "df['Keywords'] = df[['Keywords1', 'Keywords2']].apply(lambda x: ', '.join(x.dropna()), axis=1)\n",
        "\n",
        "# Now, you can drop the original 'Keywords1' and 'Keywords2' columns if they are no longer needed\n",
        "df.drop(['Keywords1', 'Keywords2'], axis=1, inplace=True)\n",
        "\n",
        "# Save the updated DataFrame to a CSV file\n",
        "filename = 'data2.csv'\n",
        "df.to_csv(filename, index=False, encoding='utf-8')\n",
        "\n",
        "df=pd.read_csv('data2.csv', encoding='utf-8')"
      ],
      "metadata": {
        "id": "hTJYCuTLhkGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Ensure that NLTK's resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Helper function to get the wordnet POS tag\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # Default to noun\n",
        "\n",
        "# Function to lemmatize text\n",
        "def lemmatize_abstract(text):\n",
        "    # Tokenize the text and get POS tags\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "\n",
        "    # Lemmatize each token with its POS tag\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(tag)) for token, tag in pos_tags]\n",
        "\n",
        "    # Join the lemmatized tokens back into a single string\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Ensure that the 'Abstract' column is a string and handle missing values by replacing NaN with an empty string\n",
        "df['Abstract'] = df['Abstract'].fillna('').astype(str)\n",
        "\n",
        "# Apply the lemmatization function to the 'Abstract' column\n",
        "df['Abstract_lemmatized'] = df['Abstract'].apply(lemmatize_abstract)\n",
        "\n",
        "# Now your DataFrame `df` has an additional column 'Abstract_lemmatized' with the lemmatized text\n"
      ],
      "metadata": {
        "id": "Xi7eIs3mhsUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Parenthesized words from 'Abstract_lemmatized'\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Assuming 'df' is your DataFrame and it contains a column 'Abstract_lemmatized'\n",
        "\n",
        "# Define a regular expression pattern to find and remove all parenthesized text\n",
        "# This pattern matches anything that starts with '(' and ends with ')', including nested parentheses\n",
        "parenthesized_text_pattern = r'\\([^()]*\\)'\n",
        "\n",
        "# Remove parenthesized content\n",
        "df['Abstract_lemmatized'] = df['Abstract_lemmatized'].str.replace(parenthesized_text_pattern, '', regex=True)\n",
        "\n",
        "# Now 'Abstract_lemmatized' will have all parenthesized text removed\n"
      ],
      "metadata": {
        "id": "LqrT8TNrknNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('data3.csv', encoding=\"utf-8\",index=False)\n",
        "df = pd.read_csv('data3.csv', encoding=\"utf-8\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "1vpGogQ8h1Bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Make sure 'Abstract_lemmatized' column exists in the DataFrame\n",
        "if 'Abstract_lemmatized' in df.columns:\n",
        "    # Fill NaN values with empty string\n",
        "    df['Abstract_lemmatized'] = df['Abstract_lemmatized'].fillna('')\n",
        "\n",
        "    # Filter abstracts containing 'digital'\n",
        "    digital_abstracts = df[df['Abstract_lemmatized'].str.contains('digital')]['Abstract_lemmatized']\n",
        "\n",
        "    # Initialize CountVectorizer, considering only bi- and tri-grams that include 'digital'\n",
        "    vectorizer = CountVectorizer(ngram_range=(2,3), stop_words='english')\n",
        "    X = vectorizer.fit_transform(digital_abstracts)\n",
        "\n",
        "    # Create a DataFrame for the co-occurrence matrix\n",
        "    co_occurrence_matrix = (X.T * X)  # This is a trick to get co-occurrence from the term-document matrix\n",
        "    co_occurrence_df = pd.DataFrame(co_occurrence_matrix.toarray(), index=vectorizer.get_feature_names_out(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "    # Since we're interested in terms with 'digital', filter the DataFrame\n",
        "    digital_co_occurrences = co_occurrence_df.filter(regex='(^|\\s)digital(\\s|$)', axis=0).filter(regex='(^|\\s)digital(\\s|$)', axis=1)\n",
        "\n",
        "    print(digital_co_occurrences)\n",
        "else:\n",
        "    print(\"The 'Abstract_lemmatized' column is not in the DataFrame.\")\n"
      ],
      "metadata": {
        "id": "x_aZjAGlUnbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Heatmap of the Top N Co-occurring Terms\n",
        "Focus on a subset of the most interesting terms that co-occur with 'digital'. You can select these based on the highest frequencies or relevance to your research question."
      ],
      "metadata": {
        "id": "iYiGLeZVVk2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "File to read again."
      ],
      "metadata": {
        "id": "iVP0oWAjdk4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "url = \"https://raw.githubusercontent.com/MK316/Workingpapers/main/DMETA/data/data1.csv\"\n",
        "df = pd.read_csv(url, encoding=\"utf-8\")\n",
        "\n",
        "df['Keywords'] = df[['Keywords1', 'Keywords2']].apply(lambda x: ', '.join(x.dropna()), axis=1)\n",
        "\n",
        "# Now, you can drop the original 'Keywords1' and 'Keywords2' columns if they are no longer needed\n",
        "df.drop(['Keywords1', 'Keywords2'], axis=1, inplace=True)\n",
        "\n",
        "# Save the updated DataFrame to a CSV file\n",
        "filename = 'data2.csv'\n",
        "df.to_csv(filename, index=False)\n",
        "\n",
        "df=pd.read_csv('data2.csv')"
      ],
      "metadata": {
        "id": "IiR44J-jbmtp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Let's say you've identified the top 20 terms that are most relevant or frequent\n",
        "top_terms = digital_co_occurrences.sum(axis=1).nlargest(20).index.tolist()\n",
        "\n",
        "# Filter the co-occurrence matrix to keep only the top terms\n",
        "filtered_matrix = digital_co_occurrences.loc[top_terms, top_terms]\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(filtered_matrix, annot=True, cmap='viridis')\n",
        "plt.title('Co-occurrence Matrix of Top 20 Terms with Digital')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-VvKsBy3gHpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimensionality reduction\n",
        "\n",
        "Apply dimensionality reduction techniques (e.g., PCA, t-SNE) to the co-occurrence matrix to visualize the relationships between terms in a 2D or 3D space."
      ],
      "metadata": {
        "id": "0KysDbXRgSlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Using PCA for dimensionality reduction\n",
        "pca = PCA(n_components=2)\n",
        "reduced_matrix = pca.fit_transform(filtered_matrix)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i, term in enumerate(filtered_matrix.index):\n",
        "    plt.scatter(reduced_matrix[i, 0], reduced_matrix[i, 1])\n",
        "    plt.text(reduced_matrix[i, 0]+0.01, reduced_matrix[i, 1]+0.01, term, fontsize=9)\n",
        "plt.title('PCA of Terms Co-occurring with Digital')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8qfeXitCgRSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "# Replace NaN values with an empty string to avoid errors with str.contains\n",
        "df['Abstract'] = df['Abstract'].fillna('')\n",
        "\n",
        "# Filter abstracts containing 'digital'\n",
        "# Now that NaN values are handled, this should not raise a ValueError\n",
        "digital_abstracts = df[df['Abstract'].str.contains('digital')]['Abstract']\n",
        "\n",
        "# Initialize CountVectorizer, considering only bi- and tri-grams that include 'digital'\n",
        "vectorizer = CountVectorizer(ngram_range=(2,3), stop_words='english')\n",
        "X = vectorizer.fit_transform(digital_abstracts)\n",
        "\n",
        "# Create a DataFrame for the co-occurrence matrix\n",
        "co_occurrence_matrix = (X.T * X)  # This is a trick to get co-occurrence from the term-document matrix\n",
        "co_occurrence_df = pd.DataFrame(co_occurrence_matrix.toarray(), index=vectorizer.get_feature_names_out(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Since we're interested in terms with 'digital', filter the DataFrame\n",
        "digital_co_occurrences = co_occurrence_df.filter(regex='(^|\\s)digital(\\s|$)', axis=0).filter(regex='(^|\\s)digital(\\s|$)', axis=1)\n",
        "\n",
        "# Print or use the digital_co_occurrences as needed\n"
      ],
      "metadata": {
        "id": "T6dFr6W6d3_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final heatmap provides a visual representation of the relationships between the most prominent terms in the corpus after consolidating similar terms and lemmatization. This can help in understanding the structure of the text data and identifying patterns of term co-occurrence."
      ],
      "metadata": {
        "id": "aynRJd_EmZ0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Make sure you've downloaded all necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Helper function to get wordnet POS tag\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return nltk.corpus.wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return nltk.corpus.wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return nltk.corpus.wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return nltk.corpus.wordnet.ADV\n",
        "    else:\n",
        "        return nltk.corpus.wordnet.NOUN\n",
        "\n",
        "# Define a function to lemmatize text\n",
        "def lemmatize_text(text):\n",
        "    # Tokenize the text\n",
        "    words = nltk.word_tokenize(text)\n",
        "    # Get POS tags for the words\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "    # Map POS tags to lemmatizer format and lemmatize\n",
        "    lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
        "    return ' '.join(lemmatized)\n",
        "\n",
        "# Manually create a mapping of terms to consolidate plural forms\n",
        "consolidation_map = {\n",
        "    'digital games': 'digital game',\n",
        "    'digital literacies': 'digital literacy',\n",
        "    # Add more terms as needed\n",
        "}\n",
        "\n",
        "# Function to consolidate terms based on the mapping\n",
        "def consolidate_terms(text):\n",
        "    for key, value in consolidation_map.items():\n",
        "        text = text.replace(key, value)\n",
        "    return text\n",
        "\n",
        "# Assuming 'df' is your DataFrame and 'Abstract' contains the text data\n",
        "# Replace NaN values with an empty string to avoid errors with str.contains\n",
        "df['Abstract'] = df['Abstract'].fillna('').astype(str)\n",
        "df['Abstract'] = df['Abstract'].str.lower()\n",
        "\n",
        "# Apply lemmatization and then term consolidation\n",
        "df['Processed_Abstract'] = df['Abstract'].apply(lemmatize_text).apply(consolidate_terms)\n",
        "\n",
        "# Create the co-occurrence matrix using CountVectorizer\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 1), stop_words='english')\n",
        "X = vectorizer.fit_transform(df['Processed_Abstract'])\n",
        "features = vectorizer.get_feature_names_out()\n",
        "co_occurrence_matrix = (X.T * X)  # This is a trick to get the co-occurrence matrix\n",
        "\n",
        "# Convert to a DataFrame for easier handling\n",
        "co_occurrence_df = pd.DataFrame(co_occurrence_matrix.toarray(), index=features, columns=features)\n",
        "\n",
        "# Filter to top 20 terms\n",
        "top_terms = co_occurrence_df.sum(axis=0).nlargest(20).index\n",
        "filtered_co_occurrence_df = co_occurrence_df.loc[top_terms, top_terms]\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(filtered_co_occurrence_df, annot=True, fmt='d', cmap='viridis')\n",
        "plt.title('Co-occurrence Matrix of Top 20 Consolidated Terms with Digital')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eeWDNm-9YFM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Network Graph\n",
        "Create a network graph where nodes represent terms and edges represent co-occurrences. This can help visualize how terms cluster around 'digital'."
      ],
      "metadata": {
        "id": "54vLe6DUVnhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Create a graph from the co-occurrence matrix\n",
        "G = nx.from_pandas_adjacency(filtered_matrix)\n",
        "\n",
        "# Use a layout that spaces nodes using the force-directed algorithm for aesthetic spacing\n",
        "pos = nx.spring_layout(G, k=0.1)\n",
        "\n",
        "plt.figure(figsize=(16, 12))\n",
        "nx.draw(G, pos, with_labels=True, node_size=4000, node_color='skyblue', font_size=10, edge_color='gray')\n",
        "plt.title('Network Graph of Top Terms Co-occurring with Digital')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-b4zlKmYVmyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part III. Thematic analysis\n",
        "\n",
        "+ Topic Modeling: Use techniques such as Latent Dirichlet Allocation (LDA) to identify prevailing topics within the abstracts. Analyze how topics related to 'digital' emerge, evolve, or decline over the years.\n",
        "+ Temporal Topic Trends: For topics strongly associated with 'digital', plot their prevalence over time to observe shifts in focus areas."
      ],
      "metadata": {
        "id": "aaMdMqy-myMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data to read\n",
        "# data4.csv (6 articles from 2024 > 2023)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "url=\"https://raw.githubusercontent.com/MK316/Workingpapers/main/DMETA/data/data4.csv\"\n",
        "\n",
        "df = pd.read_csv(url, encoding=\"utf-8\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "XILokzwCVmrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure that the 'Abstract_lemmatized' column is a string and handle missing values\n",
        "df['Abstract_lemmatized'] = df['Abstract_lemmatized'].fillna('').astype(str)\n",
        "\n",
        "# Prepare the text data and the year for each document\n",
        "text_data = df['Abstract_lemmatized'].values\n",
        "years = df['Year'].values\n",
        "\n",
        "# Create a document-term matrix\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "doc_term_matrix = vectorizer.fit_transform(text_data)\n",
        "\n",
        "# Fit the LDA model\n",
        "num_topics = 10  # Adjust the number of topics as needed\n",
        "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
        "lda.fit(doc_term_matrix)\n",
        "\n",
        "# ... the rest of the code remains the same as provided earlier\n",
        "\n",
        "\n",
        "# View the topics in LDA model\n",
        "def display_topics(model, feature_names, no_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        print(\"Topic %d:\" % (topic_idx))\n",
        "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
        "\n",
        "no_top_words = 10\n",
        "display_topics(lda, vectorizer.get_feature_names_out(), no_top_words)\n",
        "\n",
        "# Analyze how topics related to 'digital' emerge over the years\n",
        "topic_weights = lda.transform(doc_term_matrix)\n",
        "topic_over_time = pd.DataFrame({'Year': years})\n",
        "\n",
        "for i in range(num_topics):\n",
        "    topic_over_time[f'Topic {i}'] = topic_weights[:, i]\n",
        "\n",
        "# Group by year and get the mean weight of each topic\n",
        "topic_over_time = topic_over_time.groupby('Year').mean().reset_index()\n",
        "\n",
        "# Plot the temporal topic trends\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i in range(num_topics):\n",
        "    plt.plot(topic_over_time['Year'], topic_over_time[f'Topic {i}'], label=f'Topic {i}')\n",
        "\n",
        "plt.title('Temporal Topic Trends')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Mean Topic Weight')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mMkGaYG4n0PL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "e topics are not as important or relevant, consider removing them from the plot. This will reduce clutter.\n",
        "\n",
        "Aggregate Data: Instead of plotting every single year, you could aggregate the data over a larger span, such as 2-5 years, to smooth out fluctuations and make trends clearer.\n",
        "\n",
        "Focus on Major Topics: Instead of showing all topics, focus on a few major ones that are most relevant to your research question.\n",
        "\n",
        "Interactive Plot: Consider creating an interactive plot that allows you to hover over lines to see more details or to toggle the visibility of individual topics.\n",
        "\n",
        "Separate Plots: Create separate plots for each topic or group of related topics.\n",
        "\n",
        "Highlight Digital Topics: If you're specifically interested in topics related to 'digital', you could highlight these lines and use a subdued color for unrelated topics.\n",
        "\n",
        "Use Mean/median: Rather than plotting individual topic weights, consider plotting the mean or median topic weight across all topics for each year to get a sense of the overall trend.\n",
        "\n",
        "Annotations: Annotate specific points of interest, like peaks or changes in trends, to draw attention to them."
      ],
      "metadata": {
        "id": "-Qx28n7Pozwh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code will plot lines for topics related to 'digital' in brighter colors with a thicker line, while other topics will be plotted in light grey. You can adjust the digital_related_topics list to include the topics you're most interested in."
      ],
      "metadata": {
        "id": "ud2GhRuxo-j2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming topic_over_time is your DataFrame with topic weights and years\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Assuming 'digital_related_topics' is a list of topic numbers that are related to 'digital'\n",
        "digital_related_topics = [0, 2, 4, 7]  # Example: topics 0, 2, 4, 7 are related to 'digital'\n",
        "\n",
        "# Plot only the digital-related topics with brighter colors\n",
        "for topic in digital_related_topics:\n",
        "    plt.plot(topic_over_time['Year'], topic_over_time[f'Topic {topic}'], label=f'Topic {topic}', linewidth=2)\n",
        "\n",
        "# Plot other topics with a subdued color and less emphasis\n",
        "for i in range(num_topics):\n",
        "    if i not in digital_related_topics:\n",
        "        plt.plot(topic_over_time['Year'], topic_over_time[f'Topic {i}'], color='lightgrey', alpha=0.5)\n",
        "\n",
        "plt.title('Temporal Topic Trends Related to Digital')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Mean Topic Weight')\n",
        "plt.legend(title='Topic Number')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XlfwgpNao5CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to display the top words for each topic\n",
        "def display_topic_keywords(lda_model, feature_names, num_top_words):\n",
        "    for topic_idx, topic in enumerate(lda_model.components_):\n",
        "        if topic_idx in digital_related_topics:  # Check if the topic is one of the digital-related topics\n",
        "            print(f\"Topic {topic_idx}:\")\n",
        "            # Get the top words for this topic\n",
        "            top_words = [feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]\n",
        "            print(\", \".join(top_words))\n",
        "\n",
        "# Assuming you have a CountVectorizer instance 'vectorizer' that was used with LDA\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "num_top_words = 20  # Set the number of top words you want for each topic\n",
        "\n",
        "# Call the function to display the keywords\n",
        "display_topic_keywords(lda, feature_names, num_top_words)\n"
      ],
      "metadata": {
        "id": "G53pNoIVsBxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique words\n",
        "\n",
        "# Assuming lda is your fitted LDA model and vectorizer is your CountVectorizer\n",
        "\n",
        "def get_unique_keywords(lda_model, feature_names, num_keywords=20):\n",
        "    # Get the top keywords for each topic\n",
        "    top_keywords = {topic_idx: [feature_names[i]\n",
        "                                for i in topic.argsort()[:-num_keywords - 1:-1]]\n",
        "                    for topic_idx, topic in enumerate(lda_model.components_)}\n",
        "\n",
        "    # Create sets for each topic's keywords\n",
        "    keyword_sets = {topic_idx: set(keywords) for topic_idx, keywords in top_keywords.items()}\n",
        "\n",
        "    # Determine non-overlapping keywords for each topic\n",
        "    unique_keywords = {}\n",
        "    for topic_idx, keywords in keyword_sets.items():\n",
        "        # Subtract keywords from all other topics\n",
        "        other_keywords = set().union(*(s for idx, s in keyword_sets.items() if idx != topic_idx))\n",
        "        unique_keywords[topic_idx] = keywords - other_keywords\n",
        "\n",
        "    return unique_keywords\n",
        "\n",
        "# Get the unique keywords\n",
        "unique_keywords_per_topic = get_unique_keywords(lda, feature_names)\n",
        "\n",
        "# Display the unique keywords for the specified topics\n",
        "for topic in digital_related_topics:\n",
        "    print(f\"Topic {topic}:\")\n",
        "    print(\", \".join(unique_keywords_per_topic[topic]))\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "id": "TwjVrm-bsi8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample size in each year to weight"
      ],
      "metadata": {
        "id": "I4caXZ1cwcAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the number of documents for each year\n",
        "document_counts = df.groupby('Year').size()\n",
        "\n",
        "# Normalize the topic weights by the number of documents for each year\n",
        "for topic in range(num_topics):\n",
        "    column_name = f'Topic {topic}'\n",
        "    topic_over_time[column_name] = topic_over_time.apply(\n",
        "        lambda row: row[column_name] / document_counts[row['Year']], axis=1)\n",
        "\n",
        "# Now topic_over_time contains the weighted mean topic weights\n",
        "\n",
        "# You can proceed with plotting the weighted trends\n",
        "plt.figure(figsize=(15, 10))\n",
        "for topic in digital_related_topics:\n",
        "    plt.plot(topic_over_time['Year'], topic_over_time[f'Topic {topic}'], label=f'Topic {topic}')\n",
        "\n",
        "plt.title('Weighted Temporal Topic Trends Related to Digital')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Weighted Mean Topic Weight')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ofw8pa6EwfE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# First, we'll create a new column that indicates whether 'digital' is mentioned in each document\n",
        "df['Mentions_Digital'] = df['Abstract_lemmatized'].str.contains('digital')\n",
        "\n",
        "# Then, we calculate the proportion of documents mentioning 'digital' per year\n",
        "digital_proportion_per_year = df.groupby('Year')['Mentions_Digital'].mean()\n",
        "\n",
        "# Now, let's plot this trend\n",
        "plt.figure(figsize=(15, 10))\n",
        "plt.plot(digital_proportion_per_year.index, digital_proportion_per_year.values, label='Proportion of \"Digital\" Mentions', color='black', linewidth=2.5)\n",
        "\n",
        "# Optionally, overlay this with the topic trends for the related topics\n",
        "for topic in digital_related_topics:\n",
        "    plt.plot(topic_over_time['Year'], topic_over_time[f'Topic {topic}'], label=f'Topic {topic}')\n",
        "\n",
        "plt.title('Trend of \"Digital\" Mentions and Related Topics Over Time')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Proportion / Weight')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7x_Bj2OWyTGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate the raw count of documents mentioning 'digital' per year\n",
        "digital_count_per_year = df.groupby('Year')['Mentions_Digital'].sum()\n",
        "\n",
        "# Set up the figure and primary y-axis for the topic proportions\n",
        "fig, ax1 = plt.subplots(figsize=(15, 10))\n",
        "\n",
        "# Plot the topic proportions on the primary y-axis\n",
        "for topic in digital_related_topics:\n",
        "    ax1.plot(topic_over_time['Year'], topic_over_time[f'Topic {topic}'], label=f'Topic {topic}')\n",
        "\n",
        "# Set up the secondary y-axis for the raw count of 'digital' mentions\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(digital_count_per_year.index, digital_count_per_year.values, label='Raw Count of \"Digital\" Mentions', color='black', linestyle='--', linewidth=2)\n",
        "\n",
        "# Set the labels and titles\n",
        "ax1.set_xlabel('Year')\n",
        "ax1.set_ylabel('Topic Proportion')\n",
        "ax2.set_ylabel('Raw Count of \"Digital\" Mentions', color='black')\n",
        "ax1.set_title('Digital Mentions and Topic Proportions Over Time')\n",
        "\n",
        "# Add legends\n",
        "ax1_legend = ax1.legend(loc='upper left', title='Topic Number')\n",
        "ax2_legend = ax2.legend(loc='upper right', title='Digital Mentions')\n",
        "ax2.get_yaxis().set_label_coords(1.1,0.5)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RKYj0nqwy_fk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}