{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOiCkPov/UcQcTi3LpJS8R2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MK316/Workingpapers/blob/main/2025-insights/2025c_Kim%26Lee_paper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thematic analysis [1]: 2025. 7. 26"
      ],
      "metadata": {
        "id": "xmZ8EYDHauwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data to upload\n",
        "\n",
        "Notes:\n",
        "\n",
        "words replaced: Artificial intelligence = AI, applications = apps"
      ],
      "metadata": {
        "id": "abKB_z-ZQuGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ data01.csv: 33 essays with SID, Level, Essay\n",
        "+ data02.csv: 33 essays ('artificial intelligence' = AI)"
      ],
      "metadata": {
        "id": "i3MW4IY1RoU3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SqZ83ENQfF2"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Load and view the data"
      ],
      "metadata": {
        "id": "FNBBB8EtQwvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('data03.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "6NsCU6VWQ1j5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Assume df is already loaded as:\n",
        "# df = pd.read_csv('data03.csv')\n",
        "\n",
        "# Ensure the text is string type\n",
        "df['Reflection'] = df['Reflection'].astype(str)\n",
        "\n",
        "# Step 1: Replace 'artificial intelligence' with 'AI' explicitly (before lemmatization)\n",
        "df['Reflection'] = df['Reflection'].str.replace(r'\\bartificial intelligence\\b', 'AI', flags=re.IGNORECASE, regex=True)\n",
        "\n",
        "# Step 2: Replace other multi-word expressions with underscores\n",
        "multiword_terms = {\n",
        "    r'\\bdigital literacy\\b': 'digital_literacy',\n",
        "    r'\\benglish education\\b': 'english_education'\n",
        "}\n",
        "\n",
        "for pattern, replacement in multiword_terms.items():\n",
        "    df['Reflection'] = df['Reflection'].str.replace(pattern, replacement, flags=re.IGNORECASE, regex=True)\n",
        "\n",
        "# âœ… Optional preview\n",
        "df['Reflection'].head()\n"
      ],
      "metadata": {
        "id": "VqpslHzy2YZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Clean the text and lemmatize"
      ],
      "metadata": {
        "id": "8IOIB1WlRxSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Preserve this token as-is\n",
        "important_tokens = {'AI'}\n",
        "\n",
        "manual_replace = {\n",
        "    'apps': 'app',\n",
        "    'cod': 'code',         # Fix mislemmatized 'coding'\n",
        "    'coding': 'code'       # Prevent future errors if it escapes lemmatizer\n",
        "}\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "def clean_and_lemmatize_pos(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "    lemmatized_tokens = []\n",
        "    for token, tag in tagged_tokens:\n",
        "        if token in important_tokens:\n",
        "            lemmatized_tokens.append(token)  # Keep exact casing (e.g., 'AI')\n",
        "        else:\n",
        "            token = token.lower()\n",
        "            if token not in stop_words and len(token) > 2:\n",
        "                pos = get_wordnet_pos(tag)\n",
        "                lemma = lemmatizer.lemmatize(token, pos)\n",
        "                lemma = manual_replace.get(lemma, lemma)\n",
        "                lemmatized_tokens.append(lemma)\n",
        "\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Apply to the DataFrame\n",
        "df['lemmatized'] = df['Reflection'].apply(clean_and_lemmatize_pos)\n"
      ],
      "metadata": {
        "id": "e4cTV3uwR0rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "âœ… Step 3: TF-IDF with Unigrams and Bigrams"
      ],
      "metadata": {
        "id": "ka2-X-VEbdug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    min_df=2,\n",
        "    max_df=0.9,\n",
        "    stop_words='english',\n",
        "    ngram_range=(1, 1)  # unigram only\n",
        ")\n",
        "\n",
        "tfidf_matrix = vectorizer.fit_transform(df['lemmatized'])\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "tfidf_means = np.asarray(tfidf_matrix.mean(axis=0)).flatten()\n",
        "\n",
        "# View top 30\n",
        "top_n = 30\n",
        "top_indices = tfidf_means.argsort()[::-1][:top_n]\n",
        "for i in top_indices:\n",
        "    print(f\"{feature_names[i]:<30} {tfidf_means[i]:.6f}\")\n"
      ],
      "metadata": {
        "id": "sfDs22dHbgVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fix 'app-apps' 'student-students'"
      ],
      "metadata": {
        "id": "XuHfQu9cgeIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get top N words"
      ],
      "metadata": {
        "id": "NDfPFSxbbizk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 30\n",
        "top_indices = tfidf_means.argsort()[::-1][:top_n]\n",
        "for i in top_indices:\n",
        "    print(f\"{feature_names[i]:<30} {tfidf_means[i]:.6f}\")\n"
      ],
      "metadata": {
        "id": "Jryb1SnAbkmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recommended Preprocessing Order\n",
        "Lowercasing\n",
        "\n",
        "Removing non-alphabetic characters (optional, but useful)\n",
        "\n",
        "Tokenization\n",
        "\n",
        "Stopword removal\n",
        "\n",
        "Lemmatization\n",
        "\n",
        "Join back into text for TF-IDF"
      ],
      "metadata": {
        "id": "-IkcV5hBTwL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Tokenization, remove stopwords and lemmatize\n",
        "\n",
        "+ WordNetLemmatizer"
      ],
      "metadata": {
        "id": "5hqUH6s9R_vm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Display keywords"
      ],
      "metadata": {
        "id": "pQHZfxW9VcKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame of top terms and their scores\n",
        "import pandas as pd\n",
        "\n",
        "top_keywords = pd.DataFrame({\n",
        "    'term': [feature_names[i] for i in top_indices],\n",
        "    'score': [tfidf_means[i] for i in top_indices]\n",
        "})\n"
      ],
      "metadata": {
        "id": "rjO2kIU36vNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(top_keywords['term'][::-1], top_keywords['score'][::-1])\n",
        "plt.xlabel('Average TF-IDF Score')\n",
        "plt.title(f'Top {top_n} Keywords by TF-IDF')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dvray6SuVd6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Heatmap with top N keywords"
      ],
      "metadata": {
        "id": "iyDyuisTWXdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Compute average TF-IDF scores\n",
        "tfidf_means = np.asarray(tfidf_matrix.mean(axis=0)).flatten()\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "tfidf_scores = pd.DataFrame({'term': feature_names, 'score': tfidf_means})\n",
        "\n",
        "# Sort and select top 30\n",
        "top_n = 30\n",
        "top_keywords = tfidf_scores.sort_values(by='score', ascending=False).head(top_n)['term'].tolist()\n",
        "\n",
        "# Convert the sparse matrix to dense and wrap in a DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
        "\n",
        "# Keep only the top 30 keywords\n",
        "tfidf_top_df = tfidf_df[top_keywords]"
      ],
      "metadata": {
        "id": "PLHfQVP1WaFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(tfidf_top_df, cmap='YlGnBu', linewidths=0.5)\n",
        "\n",
        "plt.title(\"TF-IDF Heatmap of Top 30 Keywords Across Reflection Essays\")\n",
        "plt.xlabel(\"Keywords\")\n",
        "plt.ylabel(\"Essays\")\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the plot before showing it\n",
        "plt.savefig(\"tfidf_heatmap.png\", dpi=300)  # You can also use .pdf or .svg\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OW3vQ2CQWf74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Heatmap with scores inside"
      ],
      "metadata": {
        "id": "MoMeHKaNnYVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Set threshold for showing numbers (e.g., show only if TF-IDF score â‰¥ 0.10)\n",
        "threshold = 0.2\n",
        "\n",
        "# Create the label matrix: show score if â‰¥ threshold, else empty string\n",
        "annot_labels = tfidf_top_df.applymap(lambda x: f\"{x:.2f}\" if x >= threshold else \"\")\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(\n",
        "    tfidf_top_df,\n",
        "    cmap='YlGnBu',\n",
        "    linewidths=0.5,\n",
        "    annot=annot_labels,\n",
        "    fmt=\"\",\n",
        "    cbar=True\n",
        ")\n",
        "\n",
        "plt.title(\"TF-IDF Heatmap of Top 30 Keywords Across Reflection Essays\")\n",
        "plt.xlabel(\"Keywords\")\n",
        "plt.ylabel(\"Essays\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SJhfCyeYnaYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "nMLgae0DdV5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part II. Group similar keywords into potential themes\n",
        "\n",
        "Identify and label recurring themes across the 33 reflection essays, using a semi-automated method based on high-TF-IDF terms."
      ],
      "metadata": {
        "id": "qXW563VAdXGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### âœ… 1. Elbow Method\n",
        "This helps you choose the k (number of clusters) where the gain in performance (inertia) starts to diminish."
      ],
      "metadata": {
        "id": "VA1M-97jEtdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "inertia = []\n",
        "k_range = range(2, 11)\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(tfidf_matrix.T)  # Transpose: terms x documents\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the elbow curve\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(k_range, inertia, marker='o')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Inertia (Within-Cluster Sum of Squares)')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "plt.xticks(k_range)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ROFHm-ixEwoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### âœ… 2. Silhouette Score\n",
        "This evaluates how well-separated the clusters are. Higher score = better-defined clusters."
      ],
      "metadata": {
        "id": "s4W3imO8Ez-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "sil_scores = []\n",
        "k_range = range(2, 11)\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(tfidf_matrix.T)\n",
        "    score = silhouette_score(tfidf_matrix.T, labels)\n",
        "    sil_scores.append(score)\n",
        "\n",
        "# Plot silhouette scores\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(k_range, sil_scores, marker='o')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Analysis for Optimal k')\n",
        "plt.xticks(k_range)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-LrOWriJE2Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of themes = 3 or 4 based on the above k values"
      ],
      "metadata": {
        "id": "wviqsANqFmM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Set number of themes (you can adjust after evaluation)\n",
        "num_themes = 4\n",
        "\n",
        "# Transpose to get (terms x documents) shape\n",
        "term_matrix = tfidf_matrix.T\n",
        "\n",
        "# Apply KMeans\n",
        "kmeans = KMeans(n_clusters=num_themes, random_state=42, n_init=10)\n",
        "clusters = kmeans.fit_predict(term_matrix)\n",
        "\n",
        "# Build dataframe with terms and scores\n",
        "tfidf_scores = pd.DataFrame({\n",
        "    'term': feature_names,\n",
        "    'score': tfidf_means,\n",
        "    'cluster': clusters\n",
        "})\n",
        "\n",
        "# Sort for easy inspection\n",
        "tfidf_scores.sort_values(by=['cluster', 'score'], ascending=[True, False], inplace=True)\n",
        "\n",
        "# Display top terms per cluster\n",
        "for c in range(num_themes):\n",
        "    print(f\"\\nCluster {c}:\")\n",
        "    print(tfidf_scores[tfidf_scores['cluster'] == c].head(10)[['term', 'score']])\n"
      ],
      "metadata": {
        "id": "TvmmFUrhEJp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "k=6"
      ],
      "metadata": {
        "id": "kg686liPJ8mZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot"
      ],
      "metadata": {
        "id": "sWUPKXMdGSjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Reduce the term matrix to 2D for plotting\n",
        "pca = PCA(n_components=2)\n",
        "reduced = pca.fit_transform(term_matrix.toarray())\n",
        "\n",
        "# Plot the terms with cluster coloring\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(reduced[:, 0], reduced[:, 1], c=clusters, cmap='tab10', s=100)\n",
        "\n",
        "# Annotate each point with its term\n",
        "for i, term in enumerate(feature_names):\n",
        "    plt.annotate(term, (reduced[i, 0], reduced[i, 1]), fontsize=9)\n",
        "\n",
        "plt.title(\"TF-IDF Term Clustering (PCA-reduced)\")\n",
        "plt.xlabel(\"PCA 1\")\n",
        "plt.ylabel(\"PCA 2\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sF28kG2LGTc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Before revision"
      ],
      "metadata": {
        "id": "BFtQsanEEIDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (Optional) PCA plot"
      ],
      "metadata": {
        "id": "iqQs59889fnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Show only top keywords per cluster"
      ],
      "metadata": {
        "id": "ZRT6Glga97cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show only top N keywords per cluster\n",
        "top_n_per_cluster = 7\n",
        "\n",
        "top_keywords = (\n",
        "    tfidf_scores\n",
        "    .groupby('cluster')\n",
        "    .apply(lambda x: x.sort_values('score', ascending=False).head(top_n_per_cluster))\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# Filter PCA-reduced coordinates to just top keywords\n",
        "selected_indices = [feature_names.tolist().index(term) for term in top_keywords['term']]\n",
        "reduced_selected = reduced[selected_indices]\n",
        "cluster_selected = clusters[selected_indices]\n",
        "terms_selected = top_keywords['term'].tolist()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(reduced_selected[:, 0], reduced_selected[:, 1], c=cluster_selected, cmap='tab10', s=100)\n",
        "\n",
        "# Annotate terms\n",
        "for i, term in enumerate(terms_selected):\n",
        "    plt.annotate(term, (reduced_selected[i, 0], reduced_selected[i, 1]), fontsize=10)\n",
        "\n",
        "plt.title(\"TF-IDF Term Clustering (Top Terms Only)\")\n",
        "plt.xlabel(\"PCA 1\")\n",
        "plt.ylabel(\"PCA 2\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1JsGPz9G9-TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convex hulls to the above PCA plot"
      ],
      "metadata": {
        "id": "EymTc-z1Ckgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import ConvexHull\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Group coordinates, terms, and clusters together\n",
        "points = np.array(reduced_selected)\n",
        "labels = np.array(cluster_selected)\n",
        "terms = np.array(terms_selected)\n",
        "\n",
        "# Plot setup\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = plt.cm.tab10(np.arange(10))\n",
        "\n",
        "# Plot points per cluster with convex hulls\n",
        "for cluster_id in np.unique(labels):\n",
        "    mask = labels == cluster_id\n",
        "    cluster_points = points[mask]\n",
        "\n",
        "    # Plot points\n",
        "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], s=100,\n",
        "                color=colors[cluster_id], label=f\"Theme {cluster_id + 1}\", alpha=0.6)\n",
        "\n",
        "    # Draw convex hull if enough points\n",
        "    if len(cluster_points) >= 3:\n",
        "        hull = ConvexHull(cluster_points)\n",
        "        for simplex in hull.simplices:\n",
        "            plt.plot(cluster_points[simplex, 0], cluster_points[simplex, 1],\n",
        "                     color=colors[cluster_id], linewidth=2)\n",
        "\n",
        "    # Annotate terms\n",
        "    for (x, y), term in zip(cluster_points, terms[mask]):\n",
        "        plt.text(x, y, term, fontsize=10, ha='center', va='center')\n",
        "\n",
        "plt.title(\"TF-IDF Term Clustering (Top Terms Only) with Convex Hulls\")\n",
        "plt.xlabel(\"PCA 1\")\n",
        "plt.ylabel(\"PCA 2\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.text(x, y, term, fontsize=10, ha='center', va='center',\n",
        "         bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
        "\n",
        "plt.savefig(\"term_clustering_convex.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Y19x5QyPCpgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Avoid overlapping text"
      ],
      "metadata": {
        "id": "QlZmdvnW_aNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install adjustText"
      ],
      "metadata": {
        "id": "sf2buxtq_HSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from adjustText import adjust_text\n",
        "from scipy.spatial import ConvexHull\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Group coordinates, terms, and clusters together\n",
        "points = np.array(reduced_selected)\n",
        "labels = np.array(cluster_selected)\n",
        "terms = np.array(terms_selected)\n",
        "\n",
        "# Plot setup\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = plt.cm.tab10(np.arange(10))\n",
        "texts = []\n",
        "\n",
        "# Plot points per cluster with convex hulls\n",
        "for cluster_id in np.unique(labels):\n",
        "    mask = labels == cluster_id\n",
        "    cluster_points = points[mask]\n",
        "\n",
        "    # Plot points\n",
        "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], s=100,\n",
        "                color=colors[cluster_id], label=f\"Theme {cluster_id + 1}\", alpha=0.6)\n",
        "\n",
        "    # Draw convex hull if enough points\n",
        "    if len(cluster_points) >= 3:\n",
        "        hull = ConvexHull(cluster_points)\n",
        "        for simplex in hull.simplices:\n",
        "            plt.plot(cluster_points[simplex, 0], cluster_points[simplex, 1],\n",
        "                     color=colors[cluster_id], linewidth=2)\n",
        "\n",
        "    # Collect text objects for adjustText\n",
        "    for (x, y), term in zip(cluster_points, terms[mask]):\n",
        "        text = plt.text(x, y, term, fontsize=10, ha='center', va='center')\n",
        "        texts.append(text)\n",
        "\n",
        "# Adjust overlapping texts\n",
        "adjust_text(texts, arrowprops=dict(arrowstyle='-', color='gray', lw=0.5))\n",
        "\n",
        "plt.title(\"TF-IDF Term Clustering (Top Terms Only) with Convex Hulls and Adjusted Labels\")\n",
        "plt.xlabel(\"PCA 1\")\n",
        "plt.ylabel(\"PCA 2\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"term_clustering_convex_good.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5m3rf6BF_yFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z7m65xqrOBJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Step 2: Extract top-N terms by mean TF-IDF\n",
        "top_n = 30\n",
        "top_indices = tfidf_means.argsort()[::-1][:top_n]\n",
        "\n",
        "top_terms = feature_names[top_indices]\n",
        "top_scores = tfidf_means[top_indices]\n",
        "\n",
        "# Create new matrix with only those top terms\n",
        "tfidf_top_matrix = tfidf_matrix[:, top_indices]\n",
        "\n"
      ],
      "metadata": {
        "id": "Lt5eilf0ALJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸŽ¯ Step 3: Cluster the top terms (NOT full terms)\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "num_themes = 5\n",
        "kmeans = KMeans(n_clusters=num_themes, random_state=42)\n",
        "clusters = kmeans.fit_predict(tfidf_top_matrix.T)  # cluster terms\n",
        "\n",
        "# PCA for visualization\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "pca_result = pca.fit_transform(tfidf_top_matrix.T)\n",
        "\n"
      ],
      "metadata": {
        "id": "NMEn6tC9AUu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Step 4: Prepare plotting DataFrame\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "cluster_df = pd.DataFrame({\n",
        "    'term': top_terms,\n",
        "    'score': top_scores,\n",
        "    'cluster': clusters,\n",
        "    'x': pca_result[:, 0],\n",
        "    'y': pca_result[:, 1]\n",
        "})\n",
        "\n"
      ],
      "metadata": {
        "id": "5eENTWzDAZJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ–¼ï¸ Step 5: Plot with adjusted text (for overlap)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from adjustText import adjust_text\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
        "\n",
        "for c in range(num_themes):\n",
        "    subset = cluster_df[cluster_df['cluster'] == c]\n",
        "    plt.scatter(subset['x'], subset['y'], label=f\"Theme {c+1}\", s=60, alpha=0.7, color=colors[c])\n",
        "\n",
        "# Add keyword labels\n",
        "texts = []\n",
        "for i, row in cluster_df.iterrows():\n",
        "    texts.append(plt.text(row['x'], row['y'], row['term'], fontsize=10))\n",
        "\n",
        "adjust_text(texts, arrowprops=dict(arrowstyle='-', color='gray'))\n",
        "\n",
        "plt.title(\"Top 30 Keywords Clustered by TF-IDF Similarity (PCA + KMeans)\")\n",
        "plt.xlabel(\"PCA 1\")\n",
        "plt.ylabel(\"PCA 2\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4QptRCt4_iI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import ConvexHull\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
        "\n",
        "for c in range(num_themes):\n",
        "    subset = cluster_df[cluster_df['cluster'] == c]\n",
        "    plt.scatter(subset['x'], subset['y'], label=f\"Theme {c+1}\", color=colors[c], alpha=0.7)\n",
        "\n",
        "    # Draw convex hull\n",
        "    if len(subset) >= 3:\n",
        "        hull = ConvexHull(subset[['x', 'y']])\n",
        "        for simplex in hull.simplices:\n",
        "            plt.plot(subset['x'].values[simplex], subset['y'].values[simplex], colors[c], linewidth=2)\n",
        "\n",
        "# Label keywords\n",
        "texts = []\n",
        "for _, row in cluster_df.iterrows():\n",
        "    texts.append(plt.text(row['x'], row['y'], row['term'], fontsize=10))\n",
        "adjust_text(texts, arrowprops=dict(arrowstyle='-', color='gray'))\n",
        "\n",
        "plt.title(\"Keyword Clusters with Convex Hulls\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6Qd36J4qCV-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SQTB4v7NCVss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "themes = {}\n",
        "\n",
        "for i in range(num_themes):\n",
        "    terms_in_cluster = tfidf_scores[tfidf_scores['cluster'] == i]\n",
        "    top_terms = terms_in_cluster.sort_values(by='score', ascending=False).head(6)\n",
        "    themes[f\"Theme {i+1}\"] = list(top_terms['term'])\n",
        "\n",
        "# Print results\n",
        "for name, keywords in themes.items():\n",
        "    print(f\"{name}: {', '.join(keywords)}\")\n"
      ],
      "metadata": {
        "id": "sGPtzKdHdfrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# âœ… Next Steps After Theme Extraction"
      ],
      "metadata": {
        "id": "fAsfXaJcTDlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Interpret and Label Each Theme"
      ],
      "metadata": {
        "id": "686NspWnkn2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (name, keywords) in enumerate(themes.items(), 1):\n",
        "    print(f\"Theme {i}: {', '.join(keywords)}\")\n",
        "    # After printing, manually add:\n",
        "    # â†’ Suggested Label: e.g., \"Digital Confidence and Skill Growth\"\n"
      ],
      "metadata": {
        "id": "XcxMq3uvkpoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Visualize Theme-Keyword Relationships"
      ],
      "metadata": {
        "id": "lLahv9cmksEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add edges between theme and its keywords\n",
        "for theme, keywords in themes.items():\n",
        "    for kw in keywords:\n",
        "        G.add_edge(theme, kw)\n",
        "\n",
        "# Plot the network\n",
        "plt.figure(figsize=(10, 6))\n",
        "pos = nx.spring_layout(G, seed=42)\n",
        "nx.draw(G, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=1500, font_size=10)\n",
        "plt.title(\"Thematic Keyword Network\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fLQKZZfQkuEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "GMM trials (7/27)"
      ],
      "metadata": {
        "id": "IYAD8v2mhjVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install required libraries\n",
        "!pip install matplotlib networkx scikit-learn pandas --quiet"
      ],
      "metadata": {
        "id": "JveDP2QhiFNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from sklearn.covariance import GraphicalLassoCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# STEP 3: Upload your data\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# STEP 4: Load your CSV file\n",
        "df = pd.read_csv(list(uploaded.keys())[0])\n",
        "\n",
        "# STEP 5: Select relevant columns by name or index\n",
        "# Replace with your actual column names if different\n",
        "teaching_cols = ['Q4', 'Q5','Q6','Q7']\n",
        "learning_cols = ['Q8','Q9','Q10','Q11']\n",
        "tech_cols = ['Q12','Q13','Q14']\n",
        "\n",
        "selected_cols = teaching_cols + learning_cols + tech_cols\n",
        "data = df[selected_cols].dropna()\n",
        "\n",
        "# STEP 6: Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(data)\n",
        "\n",
        "# STEP 7: Estimate GGM using Graphical Lasso\n",
        "model = GraphicalLassoCV()\n",
        "model.fit(X)\n",
        "\n",
        "# STEP 8: Build graph from precision matrix\n",
        "precision = model.precision_\n",
        "partial_corr = -precision / np.outer(np.sqrt(np.diag(precision)), np.sqrt(np.diag(precision)))\n",
        "np.fill_diagonal(partial_corr, 0)\n",
        "\n",
        "# Threshold to remove very weak edges\n",
        "threshold = 0.1\n",
        "adjacency = (np.abs(partial_corr) > threshold).astype(int)\n",
        "\n",
        "# Create graph\n",
        "G = nx.Graph()\n",
        "labels = selected_cols\n",
        "\n",
        "# Add nodes with color group\n",
        "for i, label in enumerate(labels):\n",
        "    if label in teaching_cols:\n",
        "        G.add_node(label, group='Teaching')\n",
        "    elif label in learning_cols:\n",
        "        G.add_node(label, group='Learning')\n",
        "    else:\n",
        "        G.add_node(label, group='Tech')\n",
        "\n",
        "# Add edges with weight (partial correlation)\n",
        "for i in range(len(labels)):\n",
        "    for j in range(i + 1, len(labels)):\n",
        "        if adjacency[i, j]:\n",
        "            G.add_edge(labels[i], labels[j], weight=partial_corr[i, j])\n",
        "\n",
        "# STEP 9: Visualize with colored groups\n",
        "plt.figure(figsize=(10, 8))\n",
        "pos = nx.spring_layout(G, seed=42)\n",
        "\n",
        "# Colors for groups\n",
        "group_colors = {\n",
        "    'Teaching': '#66c2a5',\n",
        "    'Learning': '#fc8d62',\n",
        "    'Tech': '#8da0cb'\n",
        "}\n",
        "node_colors = [group_colors[G.nodes[node]['group']] for node in G.nodes]\n",
        "\n",
        "# Draw\n",
        "nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=800, alpha=0.9)\n",
        "nx.draw_networkx_labels(G, pos, font_size=10)\n",
        "nx.draw_networkx_edges(G, pos, edge_color='gray', width=1.5, alpha=0.7)\n",
        "\n",
        "plt.title(\"GGM Network of ChatGPT Survey Beliefs\", fontsize=14)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dn63LeBFhnYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Centrality?"
      ],
      "metadata": {
        "id": "a_b3xz3KlBjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install networkx\n",
        "!pip install sklearn\n",
        "!pip install pandas matplotlib seaborn numpy\n"
      ],
      "metadata": {
        "id": "wS1d9wEqlEFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Upload your CSV file (must include Q4 to Q14 columns)\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load it\n",
        "df = pd.read_csv(next(iter(uploaded)))\n",
        "\n",
        "# Filter only the relevant question columns\n",
        "selected_cols = [f'Q{i}' for i in range(4, 15)]\n",
        "data = df[selected_cols].dropna()\n"
      ],
      "metadata": {
        "id": "PpasEo2dlPS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.covariance import GraphicalLassoCV\n",
        "import numpy as np\n",
        "\n",
        "# Fit Graphical Lasso to estimate sparse inverse covariance matrix\n",
        "model = GraphicalLassoCV()\n",
        "model.fit(data)\n",
        "\n",
        "# Get partial correlation matrix\n",
        "precision = model.precision_\n",
        "d = np.sqrt(np.diag(precision))\n",
        "partial_corr = -precision / np.outer(d, d)\n",
        "np.fill_diagonal(partial_corr, 1)\n"
      ],
      "metadata": {
        "id": "IZfvSWIFlXo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Build graph from partial correlations\n",
        "threshold = 0.05  # to filter weak edges\n",
        "G = nx.Graph()\n",
        "\n",
        "# Assign clusters for coloring\n",
        "clusters = {\n",
        "    \"Teaching\": [f'Q{i}' for i in range(4, 8)],\n",
        "    \"Learning\": [f'Q{i}' for i in range(8, 12)],\n",
        "    \"Tech\": [f'Q{i}' for i in range(12, 15)],\n",
        "}\n",
        "\n",
        "color_map = {}\n",
        "for c, nodes in clusters.items():\n",
        "    for n in nodes:\n",
        "        color_map[n] = c\n",
        "\n",
        "colors = {\n",
        "    \"Teaching\": \"#66c2a5\",\n",
        "    \"Learning\": \"#fc8d62\",\n",
        "    \"Tech\": \"#8da0cb\"\n",
        "}\n",
        "\n",
        "# Add edges\n",
        "nodes = selected_cols\n",
        "for i in range(len(nodes)):\n",
        "    for j in range(i+1, len(nodes)):\n",
        "        weight = partial_corr[i, j]\n",
        "        if abs(weight) > threshold:\n",
        "            G.add_edge(nodes[i], nodes[j], weight=weight)\n",
        "\n",
        "# Add nodes with attributes\n",
        "for node in nodes:\n",
        "    G.add_node(node, group=color_map[node])\n",
        "\n",
        "# Draw the graph\n",
        "plt.figure(figsize=(8, 6))\n",
        "pos = nx.spring_layout(G, seed=42)\n",
        "node_colors = [colors[color_map[node]] for node in G.nodes()]\n",
        "nx.draw(G, pos, with_labels=True, node_color=node_colors, edge_color='gray', node_size=1000, font_size=10)\n",
        "plt.title(\"GGM Network of ChatGPT Survey Beliefs\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2S3GkFeTlarX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "centrality = nx.degree_centrality(G)\n",
        "for node, val in sorted(centrality.items(), key=lambda x: -x[1]):\n",
        "    print(f\"{node}: {val:.3f}\")\n"
      ],
      "metadata": {
        "id": "oQgBYvDSlj8u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}