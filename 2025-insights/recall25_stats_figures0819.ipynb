{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMZGf/GhYgCXKwjjZcW3xWu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MK316/Workingpapers/blob/main/2025-insights/recall25_stats_figures0819.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recall 2nd analysis (0818~)"
      ],
      "metadata": {
        "id": "u3j_g2t7HAg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "data from Drive > Research > Recall25>recalldata.csv"
      ],
      "metadata": {
        "id": "c3wAvO3LHGSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [0] Descriptive stats"
      ],
      "metadata": {
        "id": "TRGX6_6jkj7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Descriptives for Q1~Q6 (overall and by Level) ---\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Path to your CSV\n",
        "csv_path = \"/content/recalldata.csv\"   # <-- change if needed\n",
        "df = pd.read_csv(csv_path)\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "\n",
        "# Items of interest\n",
        "items = [f\"Q{i}\" for i in range(1, 7)]\n",
        "\n",
        "# Coerce Likert items to numeric (e.g., blanks -> NaN)\n",
        "for c in items:\n",
        "    if c not in df.columns:\n",
        "        raise ValueError(f\"Column '{c}' not found in the CSV.\")\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def summarize(series: pd.Series) -> dict:\n",
        "    s = series.dropna()\n",
        "    if s.empty:\n",
        "        return dict(n=0, mean=np.nan, sd=np.nan, median=np.nan,\n",
        "                    q1=np.nan, q3=np.nan, iqr=np.nan, min=np.nan, max=np.nan)\n",
        "    q1 = s.quantile(0.25)\n",
        "    q3 = s.quantile(0.75)\n",
        "    return dict(\n",
        "        n=int(s.size),\n",
        "        mean=float(s.mean()),\n",
        "        sd=float(s.std(ddof=1)),\n",
        "        median=float(s.median()),\n",
        "        q1=float(q1),\n",
        "        q3=float(q3),\n",
        "        iqr=float(q3 - q1),\n",
        "        min=float(s.min()),\n",
        "        max=float(s.max()),\n",
        "    )\n",
        "\n",
        "def build_summary(frame: pd.DataFrame, label: str | None = None) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for col in items:\n",
        "        stats = summarize(frame[col])\n",
        "        stats.update({\"item\": col})\n",
        "        if label is not None:\n",
        "            stats.update({\"Level\": label})\n",
        "        rows.append(stats)\n",
        "    out = pd.DataFrame(rows)\n",
        "    # order columns\n",
        "    cols = [\"item\",\"Level\",\"n\",\"mean\",\"sd\",\"median\",\"q1\",\"q3\",\"iqr\",\"min\",\"max\"]\n",
        "    return out[[c for c in cols if c in out.columns]]\n",
        "\n",
        "# ---------- overall ----------\n",
        "overall_summary = build_summary(df).round(2)\n",
        "print(\"Overall descriptives (Q1–Q6):\")\n",
        "display(overall_summary)\n",
        "\n",
        "# ---------- by Level ----------\n",
        "if \"Level\" not in df.columns:\n",
        "    raise ValueError(\"Column 'Level' not found. Add/rename it to use by-Level summaries.\")\n",
        "\n",
        "by_level = pd.concat(\n",
        "    [build_summary(g, lvl) for lvl, g in df.groupby(df[\"Level\"].astype(str))]\n",
        ").round(2).reset_index(drop=True)\n",
        "\n",
        "print(\"\\nDescriptives by Level (Q1–Q6):\")\n",
        "display(by_level)\n",
        "\n",
        "# ---------- optional: Likert distributions ----------\n",
        "def likert_counts(frame: pd.DataFrame, group_col: str | None = None):\n",
        "    # counts for response options 1..6\n",
        "    levels = [1,2,3,4,5,6]\n",
        "    if group_col is None:\n",
        "        out = pd.concat(\n",
        "            {col: frame[col].value_counts().reindex(levels, fill_value=0) for col in items},\n",
        "            axis=1\n",
        "        ).T\n",
        "        out.index.name = \"item\"\n",
        "        out = out.reset_index().rename(columns={i: f\"count_{i}\" for i in levels})\n",
        "        # percentages\n",
        "        out[[f\"pct_{i}\" for i in levels]] = (\n",
        "            out[[f\"count_{i}\" for i in levels]].div(out[[f\"count_{i}\" for i in levels]].sum(axis=1), axis=0).round(4)\n",
        "        )\n",
        "        return out\n",
        "    else:\n",
        "        rows = []\n",
        "        for lvl, g in frame.groupby(group_col):\n",
        "            for col in items:\n",
        "                counts = g[col].value_counts().reindex(levels, fill_value=0)\n",
        "                total = counts.sum()\n",
        "                row = {\"Level\": lvl, \"item\": col}\n",
        "                row.update({f\"count_{i}\": int(counts.get(i, 0)) for i in levels})\n",
        "                for i in levels:\n",
        "                    row[f\"pct_{i}\"] = (counts.get(i, 0) / total) if total else np.nan\n",
        "                rows.append(row)\n",
        "        return pd.DataFrame(rows)\n",
        "\n",
        "likert_overall = likert_counts(df)\n",
        "likert_byLevel = likert_counts(df, group_col=\"Level\")\n",
        "\n",
        "# ---------- save outputs ----------\n",
        "overall_summary.to_csv(\"/content/descriptives_overall_Q1_Q6.csv\", index=False)\n",
        "by_level.to_csv(\"/content/descriptives_byLevel_Q1_Q6.csv\", index=False)\n",
        "likert_overall.to_csv(\"/content/likert_counts_overall_Q1_Q6.csv\", index=False)\n",
        "likert_byLevel.to_csv(\"/content/likert_counts_byLevel_Q1_Q6.csv\", index=False)\n",
        "\n",
        "print(\"\\nSaved files:\")\n",
        "print(\"/content/descriptives_overall_Q1_Q6.csv\")\n",
        "print(\"/content/descriptives_byLevel_Q1_Q6.csv\")\n",
        "print(\"/content/likert_counts_overall_Q1_Q6.csv\")\n",
        "print(\"/content/likert_counts_byLevel_Q1_Q6.csv\")\n"
      ],
      "metadata": {
        "id": "ut1PZ01pkm3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The user wants to combine two descriptive statistics tables (overall and by level) into a single, cohesive table.\n",
        "The user also specifies the need for the table to be in APA style.\n",
        "The tables provided show the following metrics:\n",
        "\n",
        "n: number of participants\n",
        "\n",
        "mean: mean score\n",
        "\n",
        "sd: standard deviation\n",
        "\n",
        "median: median score\n",
        "\n",
        "q1: first quartile\n",
        "\n",
        "q3: third quartile\n",
        "\n",
        "iqr: interquartile range\n",
        "\n",
        "min: minimum score\n",
        "\n",
        "max: maximum score\n",
        "\n",
        "Here's how to structure the combined table in APA style:\n",
        "\n",
        "Table Number and Title: An APA table needs a table number (e.g., Table 1) and a descriptive title (in italics).\n",
        "\n",
        "Layout: The table should be clear and easy to read. Use horizontal lines to separate the table title from the header and the main body from the note, but avoid vertical lines.\n",
        "\n",
        "Grouping: The most logical way to combine these is to have a main column for the item (Q1-Q6), and then sub-columns for each group: Overall, Undergraduate (UG), and Graduate (GRAD).\n",
        "\n",
        "Metrics: Within each group's column, present the key statistics. The most common and useful for a quick summary are n, Mean, and SD. The other metrics (Median, Q1, Q3, etc.) are also important but can be simplified or placed in a note if space is a concern. For a comprehensive table, it's best to include Median and IQR as well, as they are less sensitive to outliers, which is important for Likert scale data.\n",
        "\n",
        "Notation: Use standard APA notation. M for Mean, SD for Standard Deviation, n for sample size.\n",
        "\n",
        "Note: An APA table often includes a note at the bottom to provide additional context or clarify abbreviations. This is a good place to define the scale and any other relevant details.\n",
        "\n",
        "Here is the combined table following these principles.\n",
        "\n",
        "Table for Combined Data (APA Style)\n",
        "Table 1\n",
        "Descriptive Statistics for Pre- and Post-Survey Likert Items, Overall and by Level\n",
        "\n",
        "| Item | Overall | Undergraduate (UG) | Graduate (GRAD) |\n",
        "| :--- | :---: | :---: | :---: |\n",
        "| | n = 34 | n = 18 | n = 16 |\n",
        "| | M (SD) | M (SD) | M (SD) |\n",
        "| Q1 | 2.94 (1.30) | 2.33 (1.08) | 3.62 (1.20) |\n",
        "| Q2 | 4.29 (1.09) | 4.61 (1.04) | 3.94 (1.06) |\n",
        "| Q3 | 4.74 (1.02) | 4.56 (1.10) | 4.94 (0.93) |\n",
        "| Q4 | 4.00 (1.28) | 4.33 (1.03) | 3.62 (1.45) |\n",
        "| Q5 | 5.29 (0.80) | 5.33 (0.69) | 5.25 (0.93) |\n",
        "| Q6 | 5.00 (0.95) | 5.11 (0.83) | 4.88 (1.09) |"
      ],
      "metadata": {
        "id": "YlA7LsdV9n82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic plots"
      ],
      "metadata": {
        "id": "9VObldXzl4DB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Single plot --- Overall boxplots for Q1–Q6 ---\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Path to your CSV\n",
        "csv_path = \"/content/recalldata.csv\"  # <- keep or change\n",
        "\n",
        "# Load and tidy\n",
        "df = pd.read_csv(csv_path)\n",
        "items = [f\"Q{i}\" for i in range(1, 7)]\n",
        "for c in items:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# Long form for convenience (optional)\n",
        "long = df[items].melt(var_name=\"Item\", value_name=\"Score\").dropna()\n",
        "\n",
        "# Labels to show under each box\n",
        "label_map = {\n",
        "    \"Q1\": \"Q1: DC (pre)\",\n",
        "    \"Q2\": \"Q2: PTB (pre)\",\n",
        "    \"Q3\": \"Q3: E&M (pre)\",\n",
        "    \"Q4\": \"Q4: DC (post)\",\n",
        "    \"Q5\": \"Q5: PTB (post)\",\n",
        "    \"Q6\": \"Q6: E&M (post)\",\n",
        "}\n",
        "order = items\n",
        "data = [long.loc[long[\"Item\"] == k, \"Score\"].to_numpy() for k in order]\n",
        "\n",
        "# --- Plot\n",
        "plt.figure(figsize=(12, 5), dpi=150)\n",
        "bp = plt.boxplot(\n",
        "    data,\n",
        "    labels=[label_map[k] for k in order],\n",
        "    showmeans=True,         # show mean markers\n",
        "    meanline=False,\n",
        "    showfliers=False,       # hide outliers for cleaner look on small N\n",
        "    notch=True,\n",
        "    widths=0.6,\n",
        ")\n",
        "\n",
        "# Axes formatting\n",
        "plt.ylim(1, 6.5)                          # Likert range\n",
        "plt.yticks([1, 2, 3, 4, 5, 6])\n",
        "plt.ylabel(\"Likert score (1–6)\", fontsize=14)\n",
        "plt.title(\"Overall distributions for Q1–Q6\")\n",
        "plt.grid(axis=\"y\", alpha=0.3)\n",
        "\n",
        "# Optional: overlay jittered points for visibility\n",
        "rng = np.random.default_rng(42)\n",
        "for i, y in enumerate(data, start=1):\n",
        "    x = np.full_like(y, i, dtype=float) + rng.uniform(-0.12, 0.12, size=y.size)\n",
        "    plt.scatter(x, y, s=24, alpha=0.5, edgecolor=\"none\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"/content/boxplots_overall_Q1_Q6.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Saved figure to /content/boxplots_overall_Q1_Q6.png\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "eh_wgX9Ul6Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Two plots in one area"
      ],
      "metadata": {
        "id": "g9GMg31Umw0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Order by DC, PTB, E&M"
      ],
      "metadata": {
        "id": "w2wIZ_Z9pmrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Final Two-panel boxplots (left = overall, right = by Level)\n",
        "# with item order Q1, Q4, Q2, Q5, Q3, Q6 and two-line x-axis labels.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Patch\n",
        "\n",
        "# --------- Config ---------\n",
        "csv_path = \"/content/recalldata.csv\"  # <- your file\n",
        "items_order = [\"Q1\",\"Q4\",\"Q2\",\"Q5\",\"Q3\",\"Q6\"]\n",
        "\n",
        "# Font sizes\n",
        "TITLE_FS, LABEL_FS, TICK_FS, LEGEND_FS = 16, 14, 14, 14\n",
        "\n",
        "# Two-line labels\n",
        "label_map = {\n",
        "    \"Q1\": \"Q1\\n(pre-DC)\",   \"Q4\": \"Q4\\n(post-DC)\",\n",
        "    \"Q2\": \"Q2\\n(pre-PTB)\",  \"Q5\": \"Q5\\n(post-PTB)\",\n",
        "    \"Q3\": \"Q3\\n(pre-E&M)\",  \"Q6\": \"Q6\\n(post-E&M)\",\n",
        "}\n",
        "\n",
        "# Colorblind-friendly palette (Okabe–Ito)\n",
        "palette = [\"#0072B2\", \"#D55E00\", \"#009E73\", \"#CC79A7\", \"#E69F00\", \"#56B4E9\"]\n",
        "\n",
        "# --------- Load & prepare ---------\n",
        "df = pd.read_csv(csv_path)\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "\n",
        "# Coerce Likert items to numeric\n",
        "for c in items_order:\n",
        "    if c not in df.columns:\n",
        "        raise ValueError(f\"Column '{c}' not found in the CSV.\")\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# Level needed for right panel\n",
        "if \"Level\" not in df.columns:\n",
        "    raise ValueError(\"Column 'Level' not found.\")\n",
        "df[\"Level\"] = df[\"Level\"].astype(str).str.strip()\n",
        "\n",
        "# Level ordering: UG, GRAD first if present, then others\n",
        "levels_order = []\n",
        "for lab in [\"UG\", \"GRAD\"]:\n",
        "    if lab in df[\"Level\"].unique():\n",
        "        levels_order.append(lab)\n",
        "for lab in df[\"Level\"].unique():\n",
        "    if lab not in levels_order:\n",
        "        levels_order.append(lab)\n",
        "\n",
        "# Data arrays\n",
        "overall_data = [df[q].dropna().to_numpy() for q in items_order]\n",
        "group_data = {\n",
        "    lv: [df.loc[df[\"Level\"] == lv, q].dropna().to_numpy() for q in items_order]\n",
        "    for lv in levels_order\n",
        "}\n",
        "level_colors = {lv: palette[i % len(palette)] for i, lv in enumerate(levels_order)}\n",
        "\n",
        "# --------- Plot ---------\n",
        "# Turn off tight_layout to avoid auto-squash with multi-line labels; adjust margins manually.\n",
        "plt.rcParams[\"figure.autolayout\"] = False\n",
        "fig, (ax_left, ax_right) = plt.subplots(1, 2, figsize=(15, 6), dpi=300, sharey=True)\n",
        "\n",
        "# === Left: Overall ===\n",
        "bp_left = ax_left.boxplot(\n",
        "    overall_data,\n",
        "    labels=[label_map.get(k, k) for k in items_order],\n",
        "    showmeans=True, notch=True, widths=0.6, showfliers=False, patch_artist=True\n",
        ")\n",
        "\n",
        "# Style (neutral)\n",
        "for box in bp_left['boxes']:\n",
        "    box.set(facecolor=\"#FFFFFF\", edgecolor=\"#555555\", linewidth=1.2)\n",
        "for element in ['whiskers', 'caps', 'medians']:\n",
        "    for line in bp_left[element]:\n",
        "        line.set(color=\"#555555\", linewidth=1.2)\n",
        "for mean in bp_left['means']:\n",
        "    mean.set(marker='D', markersize=4, markerfacecolor=\"#555555\", markeredgecolor=\"white\")\n",
        "\n",
        "ax_left.set_ylim(1, 6)\n",
        "ax_left.set_yticks([1, 2, 3, 4, 5, 6])\n",
        "ax_left.set_ylabel(\"Likert score (1–6)\", fontsize=LABEL_FS)\n",
        "ax_left.set_title(\"a. Overall distributions\", fontsize=TITLE_FS)\n",
        "ax_left.tick_params(axis=\"both\", labelsize=TICK_FS)\n",
        "ax_left.tick_params(axis=\"x\", pad=10)  # pad for 2-line labels\n",
        "ax_left.grid(axis=\"y\", alpha=0.3)\n",
        "\n",
        "# Optional jitter on left\n",
        "rng = np.random.default_rng(42)\n",
        "for i, y in enumerate(overall_data, start=1):\n",
        "    if len(y):\n",
        "        x = np.full_like(y, i, dtype=float) + rng.uniform(-0.12, 0.12, size=y.size)\n",
        "        ax_left.scatter(x, y, s=18, alpha=0.45, color=\"#555555\", edgecolor=\"none\")\n",
        "\n",
        "# === Right: Grouped by Level ===\n",
        "base_positions = np.arange(1, len(items_order) + 1, dtype=float)\n",
        "group_count = max(1, len(levels_order))\n",
        "offset = 0.18 if group_count == 2 else 0.25\n",
        "width  = 0.32 if group_count == 2 else 0.22\n",
        "\n",
        "legend_handles = []\n",
        "for k, lv in enumerate(levels_order):\n",
        "    if group_count == 1:\n",
        "        pos = base_positions\n",
        "    else:\n",
        "        shift = (k - (group_count - 1) / 2) * (2 * offset / (group_count - 1))\n",
        "        pos = base_positions + shift\n",
        "\n",
        "    bp = ax_right.boxplot(\n",
        "        group_data[lv],\n",
        "        positions=pos,\n",
        "        widths=width,\n",
        "        notch=True,\n",
        "        showmeans=True,\n",
        "        showfliers=False,\n",
        "        patch_artist=True\n",
        "    )\n",
        "    col = level_colors[lv]\n",
        "\n",
        "    for box in bp['boxes']:\n",
        "        box.set(facecolor=col, edgecolor=col, linewidth=1.6, alpha=0.70)\n",
        "    for line in bp['whiskers'] + bp['caps']:\n",
        "        line.set(color=col, linewidth=1.4)\n",
        "    for med in bp['medians']:\n",
        "        med.set(color=\"#222222\", linewidth=1.6)\n",
        "    for mean in bp['means']:\n",
        "        mean.set(marker='D', markersize=4, markerfacecolor=\"white\", markeredgecolor=col)\n",
        "\n",
        "    # Jitter per level (colored)\n",
        "    for j, y in enumerate(group_data[lv], start=0):\n",
        "        if len(y):\n",
        "            x = np.full_like(y, pos[j], dtype=float) + rng.uniform(-width/3, width/3, size=y.size)\n",
        "            ax_right.scatter(x, y, s=16, alpha=0.55, color=col, edgecolor=\"white\", linewidths=0.4)\n",
        "\n",
        "    legend_handles.append(Patch(facecolor=col, edgecolor=col, alpha=0.70, label=str(lv)))\n",
        "\n",
        "ax_right.set_xticks(base_positions)\n",
        "ax_right.set_xticklabels([label_map.get(k, k) for k in items_order], fontsize=TICK_FS)\n",
        "ax_right.tick_params(axis=\"x\", pad=10)  # pad for 2-line labels\n",
        "ax_right.set_ylim(0.5, 6.5)\n",
        "ax_right.set_title(\"b. Distributions by Level (Class)\", fontsize=TITLE_FS)\n",
        "ax_right.grid(axis=\"y\", alpha=0.3)\n",
        "ax_right.tick_params(axis=\"both\", labelsize=TICK_FS)\n",
        "ax_right.legend(handles=legend_handles, title=\"Level\", loc=\"lower right\",\n",
        "                frameon=False, prop={\"size\": LEGEND_FS}, title_fontsize=LEGEND_FS)\n",
        "\n",
        "# Margins so the two-line labels don't clip and both panels are visible\n",
        "plt.subplots_adjust(left=0.08, right=0.98, top=0.90, bottom=0.22, wspace=0.1)\n",
        "\n",
        "out_path = \"/content/boxplots_two_panel_Q1_Q4_Q2_Q5_Q3_Q6.png\"\n",
        "plt.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "print(f\"Saved figure to {out_path}\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "2UxaFlZprWwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [1] Wilcoxon ordinal comparison"
      ],
      "metadata": {
        "id": "8Iyfn4zJka96"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vb7DvhV-Gt8w",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown --- Colab: Wilcoxon pre–post within Level=UG/GRAD ---\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import wilcoxon, norm\n",
        "\n",
        "# ---------- Load ----------\n",
        "csv_path = \"/content/recalldata.csv\"   # <-- change to your file\n",
        "df = pd.read_csv(csv_path)\n",
        "df.columns = [c.strip() for c in df.columns]\n",
        "\n",
        "# ---------- Cohort from Level ----------\n",
        "# Expecting Level already as UG / GRAD; also normalize common variants just in case.\n",
        "df[\"Cohort\"] = (\n",
        "    df[\"Level\"].astype(str).str.strip().str.upper()\n",
        "      .replace({\"UNDERGRAD\":\"UG\",\"UNDERGRADUATE\":\"UG\",\"U\":\"UG\",\n",
        "                \"GRADUATE\":\"GRAD\",\"G\":\"GRAD\"})\n",
        ")\n",
        "print(\"Cohort counts:\\n\", df[\"Cohort\"].value_counts(dropna=False))\n",
        "\n",
        "# ---------- Likert to numeric ----------\n",
        "likert = [\"Q1\",\"Q2\",\"Q3\",\"Q4\",\"Q5\",\"Q6\"]\n",
        "for c in likert:\n",
        "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def iqr(s): return s.quantile(0.25), s.quantile(0.75)\n",
        "\n",
        "def rb_from_W(W, n_eff):\n",
        "    T = n_eff * (n_eff + 1) / 2.0\n",
        "    return (2*W/T) - 1.0\n",
        "\n",
        "def bootstrap_ci_rb(pre, post, n_boot=5000, seed=123):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    d_full = (post - pre)\n",
        "    mask = d_full != 0\n",
        "    pre, post = pre[mask], post[mask]\n",
        "    n = len(pre)\n",
        "    if n == 0:\n",
        "        return np.nan, np.nan\n",
        "    T = n * (n + 1) / 2.0\n",
        "    vals = np.empty(n_boot)\n",
        "    for b in range(n_boot):\n",
        "        idx = rng.integers(0, n, n)\n",
        "        d = post[idx] - pre[idx]\n",
        "        ranks = pd.Series(np.abs(d)).rank(method=\"average\").to_numpy()\n",
        "        W_plus  = ranks[d > 0].sum()\n",
        "        W_minus = ranks[d < 0].sum()\n",
        "        vals[b] = (W_plus - W_minus) / T\n",
        "    lo, hi = np.percentile(vals, [2.5, 97.5])\n",
        "    return float(lo), float(hi)\n",
        "\n",
        "def describe(x):\n",
        "    x = pd.Series(x).dropna()\n",
        "    if len(x)==0:\n",
        "        return dict(n=0, median=np.nan, q1=np.nan, q3=np.nan, mean=np.nan, sd=np.nan)\n",
        "    q1, q3 = x.quantile(0.25), x.quantile(0.75)\n",
        "    return dict(n=len(x), median=float(x.median()), q1=float(q1), q3=float(q3),\n",
        "                mean=float(x.mean()), sd=float(x.std(ddof=1)))\n",
        "\n",
        "def analyze_pair(df_cohort, pre_col, post_col, construct, cohort):\n",
        "    sub = df_cohort[[pre_col, post_col]].dropna().astype(float)\n",
        "    pre = sub[pre_col].to_numpy()\n",
        "    post = sub[post_col].to_numpy()\n",
        "\n",
        "    pre_d, post_d = describe(pre), describe(post)\n",
        "    if len(sub) == 0:\n",
        "        return {**{\"cohort\":cohort,\"construct\":construct},\n",
        "                **{k:np.nan for k in [\n",
        "                    \"pre_n\",\"pre_median\",\"pre_Q1\",\"pre_Q3\",\n",
        "                    \"post_n\",\"post_median\",\"post_Q1\",\"post_Q3\",\n",
        "                    \"W\",\"p_value\",\"r_rb\",\"r_rb_CI_lo\",\"r_rb_CI_hi\",\"r_from_Z\"\n",
        "                ]}}\n",
        "\n",
        "    # p-value (two-sided). Order as post, pre so “improvement” is post > pre\n",
        "    res = wilcoxon(post, pre, zero_method=\"wilcox\", alternative=\"two-sided\", method=\"auto\")\n",
        "    W = float(res.statistic); p = float(res.pvalue)\n",
        "\n",
        "    # Effect size from signed ranks\n",
        "    d = post - pre\n",
        "    mask = d != 0\n",
        "    d = d[mask]\n",
        "    n_eff = int(len(d))\n",
        "    if n_eff == 0:\n",
        "        r_rb = r_lo = r_hi = r_from_Z = np.nan\n",
        "    else:\n",
        "        ranks = pd.Series(np.abs(d)).rank(method=\"average\").to_numpy()\n",
        "        W_plus  = ranks[d > 0].sum()\n",
        "        W_minus = ranks[d < 0].sum()\n",
        "        T = n_eff * (n_eff + 1) / 2.0\n",
        "        r_rb = (W_plus - W_minus) / T                 # positive if post > pre\n",
        "        r_lo, r_hi = bootstrap_ci_rb(pre, post)       # same orientation\n",
        "        z_abs = norm.isf(p/2.0) if 0 < p < 1 else np.nan\n",
        "        r_from_Z = float(np.sign(r_rb) * z_abs / np.sqrt(n_eff)) if np.isfinite(z_abs) else np.nan\n",
        "\n",
        "    return {\n",
        "        \"cohort\": cohort, \"construct\": construct,\n",
        "        \"pre_n\": pre_d[\"n\"], \"pre_median\": pre_d[\"median\"], \"pre_Q1\": pre_d[\"q1\"], \"pre_Q3\": pre_d[\"q3\"],\n",
        "        \"post_n\": post_d[\"n\"], \"post_median\": post_d[\"median\"], \"post_Q1\": post_d[\"q1\"], \"post_Q3\": post_d[\"q3\"],\n",
        "        \"W\": W, \"p_value\": p, \"r_rb\": r_rb, \"r_rb_CI_lo\": r_lo, \"r_rb_CI_hi\": r_hi, \"r_from_Z\": r_from_Z\n",
        "    }\n",
        "\n",
        "# ---------- Run analyses ----------\n",
        "constructs = {\n",
        "    \"Digital confidence (DC)\": (\"Q1\", \"Q4\"),\n",
        "    \"Perceived teaching benefit (PTB)\": (\"Q2\", \"Q5\"),\n",
        "    \"Engagement & motivation (E&M)\": (\"Q3\", \"Q6\"),\n",
        "}\n",
        "\n",
        "rows = []\n",
        "for cohort in [\"UG\",\"GRAD\"]:\n",
        "    g = df[df[\"Cohort\"] == cohort]\n",
        "    for name, (pre_col, post_col) in constructs.items():\n",
        "        rows.append(analyze_pair(g, pre_col, post_col, name, cohort))\n",
        "\n",
        "summary = pd.DataFrame(rows)\n",
        "summary\n",
        "summary.to_csv(\"/content/survey_wilcoxon_summary.csv\", index=False)\n",
        "print(\"Saved to /content/survey_wilcoxon_summary.csv\")\n"
      ]
    }
  ]
}