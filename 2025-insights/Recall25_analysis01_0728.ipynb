{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPBU4U2rG2asaMyqrV9zbbd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MK316/Workingpapers/blob/main/2025-insights/Recall25_analysis01_0728.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data analysis (25. 07.28)\n",
        "\n",
        "+ data: 02_combined.csv (Drive/Research/Recall25/)\n",
        "+ Q1~Q6\n",
        "+ Questionnaire construct: DC(Q1, Q4), PTB(Q2, Q4), E&M(Q3, Q6).\n",
        "+ Digital Competence, Perceived Teaching Benefits, Engagement & Motivation"
      ],
      "metadata": {
        "id": "SVbPqlsVtKdh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9I7L67gtKCS"
      },
      "outputs": [],
      "source": [
        "# Step 1: Install and import required libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Step 2: Mount Google Drive (if file is stored there)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Load your CSV file (adjust path if necessary)\n",
        "# Example path: '/content/drive/MyDrive/survey_data/02_combined.csv'\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Research/Recall25/02_combined.csv'  # <-- Change to your actual path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Select only relevant columns\n",
        "cols_to_use = ['SID','Group', 'Level', 'Age', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6','Essay-E']\n",
        "df = pd.read_csv(file_path, usecols=cols_to_use)\n",
        "\n",
        "# Preview selected columns\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "54ay845xuJ1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descriptive stats"
      ],
      "metadata": {
        "id": "jF6I2qNqu9qT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Descriptive stats for Level and Age by Group\n",
        "level_age_stats = df.groupby('Group')[['Level', 'Age']].describe()\n",
        "print(\"Descriptive statistics for Level and Age by Group:\")\n",
        "display(level_age_stats)"
      ],
      "metadata": {
        "id": "ey97zNp3u_i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Descriptive stats for Q1~Q6 by Group\n",
        "q_cols = ['Q1', 'Q2', 'Q3']\n",
        "q_stats = df.groupby('Group')[q_cols].describe()\n",
        "print(\"Descriptive statistics for Q1~Q3 by Group:\")\n",
        "display(q_stats)"
      ],
      "metadata": {
        "id": "DZt_ST1GvXYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Descriptive stats for Q1~Q6 by Group\n",
        "q_cols = ['Q4', 'Q5', 'Q6']\n",
        "q_stats = df.groupby('Group')[q_cols].describe()\n",
        "print(\"Descriptive statistics for Q4~Q6 by Group:\")\n",
        "display(q_stats)"
      ],
      "metadata": {
        "id": "bkbqnBZMve1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Descriptive stats for Q4~Q6 by Group\n",
        "q_cols = ['Q4', 'Q5', 'Q6']\n",
        "q_stats = df.groupby('Group')[q_cols].describe()\n",
        "\n",
        "# Drop quantiles: 25%, 50%, 75%\n",
        "q_stats = q_stats.drop(columns=[('Q4', '25%'), ('Q4', '50%'), ('Q4', '75%'),\n",
        "                                ('Q5', '25%'), ('Q5', '50%'), ('Q5', '75%'),\n",
        "                                ('Q6', '25%'), ('Q6', '50%'), ('Q6', '75%')])\n",
        "\n",
        "print(\"Descriptive statistics for Q4~Q6 by Group (quantiles hidden):\")\n",
        "display(q_stats)\n"
      ],
      "metadata": {
        "id": "1Q4x9rleCWaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting"
      ],
      "metadata": {
        "id": "NvUHNuKRwq6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Boxplots A and B groups (works well)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Step 1: Load data\n",
        "file_path = '/content/drive/MyDrive/Research/Recall25/02_combined.csv'\n",
        "cols_to_use = ['Group', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6']\n",
        "df = pd.read_csv(file_path, usecols=cols_to_use)\n",
        "\n",
        "# Step 2: Melt the data to long format for pre-post pairing\n",
        "df_long = pd.melt(df,\n",
        "                  id_vars='Group',\n",
        "                  value_vars=['Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6'],\n",
        "                  var_name='Question',\n",
        "                  value_name='Response')\n",
        "\n",
        "# Step 3: Add a 'Pair' column and a 'Time' column\n",
        "question_map = {\n",
        "    'Q1': ('DC', 'Pre'), 'Q4': ('DC', 'Post'),\n",
        "    'Q2': ('PTB', 'Pre'), 'Q5': ('PTB', 'Post'),\n",
        "    'Q3': ('E&M', 'Pre'), 'Q6': ('E&M', 'Post')\n",
        "}\n",
        "df_long['Pair'] = df_long['Question'].map(lambda x: question_map[x][0])\n",
        "df_long['Time'] = df_long['Question'].map(lambda x: question_map[x][1])\n",
        "\n",
        "# Step 4: Filter by group and plot boxplots\n",
        "for group in df_long['Group'].unique():\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    subset = df_long[df_long['Group'] == group]\n",
        "    sns.boxplot(data=subset, x='Pair', y='Response', hue='Time', palette='pastel')\n",
        "    plt.title(f'Pre vs Post Responses by Pair – {group} Group')\n",
        "    plt.ylim(0, 7)\n",
        "    plt.ylabel('Likert Scale (1~6)')\n",
        "    plt.xlabel('Construct')\n",
        "    plt.legend(title='Time')\n",
        "    plt.grid(True, linestyle='--', alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"Boxplot_{group}.png\", dpi=300)\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "slFbnl3LwsVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot as categorical data"
      ],
      "metadata": {
        "id": "O5s9bd42xcm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Stacked bar plot (works but ugly)\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your data\n",
        "file_path = '/content/drive/MyDrive/Research/Recall25/02_combined.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Question-to-pair mapping\n",
        "question_map = {\n",
        "    'Q1': ('DC', 'Pre'), 'Q4': ('DC', 'Post'),\n",
        "    'Q2': ('PTB', 'Pre'), 'Q5': ('PTB', 'Post'),\n",
        "    'Q3': ('E&M', 'Pre'), 'Q6': ('E&M', 'Post')\n",
        "}\n",
        "\n",
        "# Reshape\n",
        "records = []\n",
        "for col, (pair, time) in question_map.items():\n",
        "    temp = df[['Group', col]].copy()\n",
        "    temp.columns = ['Group', 'Response']\n",
        "    temp['Pair'] = pair\n",
        "    temp['Time'] = time\n",
        "    records.append(temp)\n",
        "\n",
        "df_long = pd.concat(records)\n",
        "\n",
        "# Summarize\n",
        "counts = df_long.groupby(['Group', 'Pair', 'Time', 'Response']).size().unstack(fill_value=0)\n",
        "counts_pct = counts.div(counts.sum(axis=1), axis=0) * 100\n",
        "\n",
        "# Desired order for x-axis\n",
        "desired_order = [('DC', 'Pre'), ('DC', 'Post'), ('PTB', 'Pre'), ('PTB', 'Post'), ('E&M', 'Pre'), ('E&M', 'Post')]\n",
        "\n",
        "# Plot\n",
        "for group in counts_pct.index.get_level_values(0).unique():\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "    group_data = counts_pct.loc[group]\n",
        "\n",
        "    # Reindex to force horizontal order\n",
        "    group_data = group_data.reindex(desired_order)\n",
        "\n",
        "    bottom = pd.Series(0, index=group_data.index)\n",
        "    for resp in range(1, 7):\n",
        "        if resp in group_data.columns:\n",
        "            values = group_data[resp]\n",
        "        else:\n",
        "            values = pd.Series(0, index=group_data.index)\n",
        "        ax.bar(\n",
        "            x=[f\"{pair} ({time})\" for pair, time in group_data.index],\n",
        "            height=values,\n",
        "            bottom=bottom,\n",
        "            label=f'{resp}',\n",
        "            width=0.6\n",
        "        )\n",
        "        bottom += values\n",
        "\n",
        "    ax.set_title(f'Likert Response Distribution – {group} Group')\n",
        "    ax.set_ylabel('Percentage (%)')\n",
        "    ax.set_xlabel('Construct (Time)')\n",
        "    ax.legend(title='Likert Scale', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    ax.set_ylim(0, 100)\n",
        "    plt.xticks(rotation=30)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TewhHvUaxf1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Slope graph"
      ],
      "metadata": {
        "id": "5WblJPgj0Dt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Line plot: 3 constructs * 2 groups all\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load data\n",
        "file_path = '/content/drive/MyDrive/Research/Recall25/02_combined.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Construct and time mapping\n",
        "mean_map = {\n",
        "    'Q1': ('DC', 'Pre'), 'Q4': ('DC', 'Post'),\n",
        "    'Q2': ('PTB', 'Pre'), 'Q5': ('PTB', 'Post'),\n",
        "    'Q3': ('E&M', 'Pre'), 'Q6': ('E&M', 'Post')\n",
        "}\n",
        "\n",
        "# Reshape data\n",
        "slope_data = []\n",
        "for q, (pair, time) in mean_map.items():\n",
        "    temp = df[['Group', q]].copy()\n",
        "    temp.columns = ['Group', 'Score']\n",
        "    temp['Construct'] = pair\n",
        "    temp['Time'] = time\n",
        "    slope_data.append(temp)\n",
        "\n",
        "df_slope = pd.concat(slope_data)\n",
        "\n",
        "# Ensure Time is ordered: Pre → Post\n",
        "df_slope['Time'] = pd.Categorical(df_slope['Time'], categories=['Pre', 'Post'], ordered=True)\n",
        "# Replace group codes for clarity\n",
        "df_slope['Group'] = df_slope['Group'].replace({'A': 'UG', 'B': 'GRAD'})\n",
        "# Compute means\n",
        "mean_df = df_slope.groupby(['Group', 'Construct', 'Time'])['Score'].mean().reset_index()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(data=mean_df, x='Time', y='Score', hue='Construct', style='Group',\n",
        "             markers=True, dashes=False)\n",
        "\n",
        "plt.title('Mean Likert Score Change (Pre to Post)')\n",
        "plt.ylim(1, 6)\n",
        "plt.ylabel('Mean Score (1–6)')\n",
        "plt.xlabel('Time')\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.legend(title='Construct / Group', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "cD1SXkrK0HVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split line plots by group"
      ],
      "metadata": {
        "id": "_x7OGwXp1YDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Line plot works\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load your data\n",
        "file_path = '/content/drive/MyDrive/Research/Recall25/02_combined.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Prepare long-format data\n",
        "question_map = {\n",
        "    'Q1': ('DC', 'Pre'), 'Q4': ('DC', 'Post'),\n",
        "    'Q2': ('PTB', 'Pre'), 'Q5': ('PTB', 'Post'),\n",
        "    'Q3': ('E&M', 'Pre'), 'Q6': ('E&M', 'Post')\n",
        "}\n",
        "\n",
        "records = []\n",
        "for col, (construct, time) in question_map.items():\n",
        "    temp = df[['Group', col]].copy()\n",
        "    temp.columns = ['Group', 'Score']\n",
        "    temp['Construct'] = construct\n",
        "    temp['Time'] = time\n",
        "    records.append(temp)\n",
        "\n",
        "df_long = pd.concat(records)\n",
        "\n",
        "# Fix group names\n",
        "df_long['Group'] = df_long['Group'].replace({'A': 'UG', 'B': 'GRAD'})\n",
        "\n",
        "# Fix Time order explicitly\n",
        "df_long['Time'] = pd.Categorical(df_long['Time'], categories=['Pre', 'Post'], ordered=True)\n",
        "\n",
        "# Compute group means\n",
        "df_mean = df_long.groupby(['Group', 'Construct', 'Time'])['Score'].mean().reset_index()\n",
        "\n",
        "# Create side-by-side plots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 7), sharey=True)\n",
        "\n",
        "for i, group in enumerate(['UG', 'GRAD']):\n",
        "    ax = axes[i]\n",
        "    sns.lineplot(\n",
        "        data=df_mean[df_mean['Group'] == group],\n",
        "        x='Time', y='Score', hue='Construct',\n",
        "        marker='o', linewidth=4, ax=ax  # You can adjust linewidth as needed\n",
        "    )\n",
        "\n",
        "    ax.set_title(f'{group} Group', fontsize=16)\n",
        "    ax.set_ylim(1, 6)\n",
        "    ax.set_ylabel('Mean Score (1–6)' if i == 0 else '', fontsize=14)\n",
        "    ax.set_xlabel('Time', fontsize=14)\n",
        "    ax.tick_params(axis='both', labelsize=12)\n",
        "    ax.grid(True, linestyle='--', alpha=0.5)\n",
        "    ax.legend(title='Construct', fontsize=10, loc='lower left', title_fontsize=18)\n",
        "    if i == 1:\n",
        "        ax.get_legend().set_title('Construct')\n",
        "    else:\n",
        "        ax.legend_.remove()\n",
        "\n",
        "plt.suptitle('Mean Likert Score Change (Pre to Post)', fontsize=14)\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fno1Ql-g1bBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Error bar plot"
      ],
      "metadata": {
        "id": "1guAcZtY41Us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_long.columns)\n"
      ],
      "metadata": {
        "id": "xUcdIVzYNm-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Plot (working but the one below is better)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Replace abbreviations (spelling fixed)\n",
        "df_long['Construct'] = df_long['Construct'].replace({\n",
        "    'DC': 'Digital Confidence',\n",
        "    'PTB': 'Perceived Teaching Benefits',  # Fixed spelling\n",
        "    'E&M': 'Engagement & Motivation'\n",
        "})\n",
        "\n",
        "# Confirm unique constructs to match styles\n",
        "print(df_long['Construct'].unique())  # Should show exactly 3 labels\n",
        "\n",
        "# Plot\n",
        "# ✅ Corrected plotting code\n",
        "g = sns.catplot(\n",
        "    data=df_long,\n",
        "    x='Time',\n",
        "    y='Score',  # <-- fixed here\n",
        "    hue='Construct',\n",
        "    col='Group',\n",
        "    kind='point',\n",
        "    dodge=0.3,\n",
        "    ci=95,\n",
        "    markers=['o', 's', 'D'],\n",
        "    linestyles=['-.', '-', '--'],\n",
        "    palette='Set1',\n",
        "    height=4,\n",
        "    aspect=1.5\n",
        ")\n",
        "\n",
        "\n",
        "g.set_titles(\"{col_name} Group\")\n",
        "g.set_axis_labels(\"Time\", \"Mean Score (1–6)\")\n",
        "g.set(ylim=(0, 6))\n",
        "g.fig.suptitle(\"Construct-wise Response Change with 95% CI\", fontsize=14)\n",
        "\n",
        "# Adjust title/legend spacing\n",
        "g.fig.subplots_adjust(top=0.85, right=0.75)\n",
        "\n",
        "# Legend formatting\n",
        "g._legend.set_bbox_to_anchor((0.54, 0.28))\n",
        "g._legend.set_loc('center left')\n",
        "g._legend.set_title(\"Construct\")\n",
        "g._legend.set_frame_on(True)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EYboOIQV43Nf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Load your data\n",
        "file_path = '/content/drive/MyDrive/Research/Recall25/02_combined.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Prepare long-format data\n",
        "question_map = {\n",
        "    'Q1': ('DC', 'Pre'), 'Q4': ('DC', 'Post'),\n",
        "    'Q2': ('PTB', 'Pre'), 'Q5': ('PTB', 'Post'),\n",
        "    'Q3': ('E&M', 'Pre'), 'Q6': ('E&M', 'Post')\n",
        "}\n",
        "\n",
        "records = []\n",
        "for col, (construct, time) in question_map.items():\n",
        "    temp = df[['Group', col]].copy()\n",
        "    temp.columns = ['Group', 'Response']\n",
        "    temp['Construct'] = construct\n",
        "    temp['Time'] = time\n",
        "    records.append(temp)\n",
        "\n",
        "df_long = pd.concat(records)\n",
        "\n",
        "# Replace abbreviations BEFORE plotting\n",
        "df_long['Construct'] = df_long['Construct'].replace({\n",
        "    'DC': 'Digital Confidence',\n",
        "    'PTB': 'Perceived Teaching Benefits',\n",
        "    'E&M': 'Engagement & Motivation'\n",
        "})\n",
        "\n",
        "df_long['Group'] = df_long['Group'].replace({\n",
        "    'A': 'Course A: Undergraduate',\n",
        "    'B': 'Course B: Graduate'\n",
        "})\n",
        "\n",
        "# Create catplot\n",
        "g = sns.catplot(\n",
        "    data=df_long,\n",
        "    x='Time',\n",
        "    y='Response',\n",
        "    hue='Construct',\n",
        "    col='Group',\n",
        "    kind='point',\n",
        "    dodge=0.3,\n",
        "    errorbar=('ci', 95),  # updated from deprecated ci=\n",
        "    markers=['o', 's', 'D'],\n",
        "    linestyles=['-.', '-', '--'],\n",
        "    palette='Set1',\n",
        "    height=4,\n",
        "    aspect=1.6\n",
        ")\n",
        "\n",
        "# Update figure size to ensure space\n",
        "g.fig.set_size_inches(12, 5)\n",
        "\n",
        "# Set titles and labels\n",
        "g.set_titles(\"{col_name} Group\")\n",
        "g.set_axis_labels(\"Time\", \"Mean Score (1–6)\")\n",
        "g.set(ylim=(0, 6))\n",
        "g.fig.suptitle(\"Construct-wise Response Change with 95% CI\", fontsize=14)\n",
        "\n",
        "# Remove default legend\n",
        "g._legend.remove()\n",
        "\n",
        "# Add a custom legend below the plot\n",
        "handles, labels = g.axes[0][0].get_legend_handles_labels()\n",
        "legend = g.fig.legend(\n",
        "    handles=handles,\n",
        "    labels=labels,\n",
        "    loc='lower center',\n",
        "    bbox_to_anchor=(0.5, -0.05),  # place legend below the axes\n",
        "    ncol=3,\n",
        "    title='Construct',\n",
        "    frameon=True\n",
        ")\n",
        "\n",
        "# Final layout adjustment\n",
        "g.fig.tight_layout()\n",
        "g.fig.subplots_adjust(top=0.85, bottom=0.2)\n",
        "  # make room for title and legend\n",
        "g.fig.savefig(\"construct_response_plot.png\", dpi=300, bbox_inches='tight')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3yy9fOja9RVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Thematic analysis"
      ],
      "metadata": {
        "id": "vIe2QRCISYyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Load your CSV file (adjust path if necessary)\n",
        "# Example path: '/content/drive/MyDrive/survey_data/02_combined.csv'\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Research/Recall25/02_combined.csv'  # <-- Change to your actual path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Select only relevant columns\n",
        "cols_to_use = ['SID','Group', 'Level', 'Age', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6','Essay-E']\n",
        "df = pd.read_csv(file_path, usecols=cols_to_use)\n",
        "\n",
        "# Preview selected columns\n",
        "df.head()"
      ],
      "metadata": {
        "id": "JlXvcbAYSbKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Step 2: Preprocess the text\n",
        "We’ll use basic cleaning and tokenization with TfidfVectorizer."
      ],
      "metadata": {
        "id": "UXiQL1G3S_A9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Basic pre-processing (stopwords removal is done inside vectorizer)\n",
        "vectorizer = TfidfVectorizer(\n",
        "    stop_words='english',\n",
        "    lowercase=True,\n",
        "    max_df=0.8,\n",
        "    min_df=2,\n",
        "    ngram_range=(1, 2)  # unigrams and bigrams\n",
        ")\n",
        "\n",
        "# Create a new column to ensure no missing values\n",
        "df['Essay-E'] = df['Essay-E'].fillna('')\n",
        "\n",
        "# Fit TF-IDF on all essays\n",
        "tfidf_matrix = vectorizer.fit_transform(df['Essay-E'])\n",
        "\n",
        "# Get feature names\n",
        "terms = vectorizer.get_feature_names_out()\n"
      ],
      "metadata": {
        "id": "FMQNdeqDS4M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Step 3: Separate TF-IDF matrices by group"
      ],
      "metadata": {
        "id": "w8c6my6IS9kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get group indices\n",
        "group_a_idx = df[df['Group'] == 'A'].index\n",
        "group_b_idx = df[df['Group'] == 'B'].index\n",
        "\n",
        "# Subset the TF-IDF matrix\n",
        "tfidf_a = tfidf_matrix[group_a_idx]\n",
        "tfidf_b = tfidf_matrix[group_b_idx]\n",
        "\n",
        "# Compute mean TF-IDF scores per term for each group\n",
        "mean_a = np.asarray(tfidf_a.mean(axis=0)).flatten()\n",
        "mean_b = np.asarray(tfidf_b.mean(axis=0)).flatten()\n"
      ],
      "metadata": {
        "id": "auQJY7a9S7Xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Step 4: Create a comparison DataFrame\n",
        "\n",
        "This table helps you identify:\n",
        "\n",
        "Terms more frequently used by Group A (diff > 0)\n",
        "\n",
        "Terms more frequently used by Group B (diff < 0)"
      ],
      "metadata": {
        "id": "YUBtePSqTGUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comparison_df = pd.DataFrame({\n",
        "    'term': terms,\n",
        "    'mean_A': mean_a,\n",
        "    'mean_B': mean_b,\n",
        "    'diff': mean_a - mean_b\n",
        "})\n",
        "\n",
        "# Sort by absolute difference\n",
        "comparison_df['abs_diff'] = comparison_df['diff'].abs()\n",
        "comparison_df_sorted = comparison_df.sort_values(by='abs_diff', ascending=False)\n"
      ],
      "metadata": {
        "id": "iUu-liNXTHER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Step 5: Visualize key differences (optional)"
      ],
      "metadata": {
        "id": "pEuimI6fTO3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "top_n = 15\n",
        "top_diff = comparison_df_sorted.head(top_n)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(top_diff['term'], top_diff['diff'], color=(top_diff['diff'] > 0).map({True: 'blue', False: 'orange'}))\n",
        "plt.axvline(0, color='gray', linewidth=1)\n",
        "plt.xlabel(\"TF-IDF Difference (A - B)\")\n",
        "plt.title(\"Top Keyword Differences Between Group A and B\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "d5YKbVspTPsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thematic analysis - with actual data"
      ],
      "metadata": {
        "id": "YXpc573SUAEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Assume df is already loaded as:\n",
        "# df = pd.read_csv('data03.csv')\n",
        "\n",
        "# Ensure the text is string type\n",
        "df['Essay-E'] = df['Essay-E'].astype(str)\n",
        "\n",
        "# Step 1: Replace 'artificial intelligence' with 'AI' explicitly (before lemmatization)\n",
        "df['Essay-E'] = df['Essay-E'].str.replace(r'\\bartificial intelligence\\b', 'AI', flags=re.IGNORECASE, regex=True)\n",
        "\n",
        "# Step 2: Replace other multi-word expressions with underscores\n",
        "multiword_terms = {\n",
        "    r'\\bdigital literacy\\b': 'digital_literacy',\n",
        "    r'\\benglish education\\b': 'english_education'\n",
        "}\n",
        "\n",
        "for pattern, replacement in multiword_terms.items():\n",
        "    df['Essay-E'] = df['Essay-E'].str.replace(pattern, replacement, flags=re.IGNORECASE, regex=True)\n",
        "\n",
        "# ✅ Optional preview\n",
        "df['Essay-E'].head()"
      ],
      "metadata": {
        "id": "9cq3zhGOUD_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Clean the text and lemmatize"
      ],
      "metadata": {
        "id": "8IOIB1WlRxSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet, stopwords\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources (run once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Preserve this token as-is\n",
        "important_tokens = {'AI'}\n",
        "\n",
        "manual_replace = {\n",
        "    'apps': 'app',\n",
        "    'cod': 'code',         # Fix mislemmatized 'coding'\n",
        "    'coding': 'code'       # Prevent future errors if it escapes lemmatizer\n",
        "}\n",
        "\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "def clean_and_lemmatize_pos(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "    lemmatized_tokens = []\n",
        "    for token, tag in tagged_tokens:\n",
        "        if token in important_tokens:\n",
        "            lemmatized_tokens.append(token)  # Keep exact casing (e.g., 'AI')\n",
        "        else:\n",
        "            token = token.lower()\n",
        "            if token not in stop_words and len(token) > 2:\n",
        "                pos = get_wordnet_pos(tag)\n",
        "                lemma = lemmatizer.lemmatize(token, pos)\n",
        "                lemma = manual_replace.get(lemma, lemma)\n",
        "                lemmatized_tokens.append(lemma)\n",
        "\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Apply to the DataFrame\n",
        "df['lemmatized'] = df['Essay-E'].apply(clean_and_lemmatize_pos)\n"
      ],
      "metadata": {
        "id": "e4cTV3uwR0rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "pJUTvalyUnAu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ Step 3: TF-IDF with Unigrams and Bigrams"
      ],
      "metadata": {
        "id": "ka2-X-VEbdug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    min_df=2,\n",
        "    max_df=0.9,\n",
        "    stop_words='english',\n",
        "    ngram_range=(1, 1)  # unigram only\n",
        ")\n",
        "\n",
        "tfidf_matrix = vectorizer.fit_transform(df['lemmatized'])\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "tfidf_means = np.asarray(tfidf_matrix.mean(axis=0)).flatten()\n",
        "\n",
        "# View top 30\n",
        "top_n = 30\n",
        "top_indices = tfidf_means.argsort()[::-1][:top_n]\n",
        "for i in top_indices:\n",
        "    print(f\"{feature_names[i]:<30} {tfidf_means[i]:.6f}\")\n"
      ],
      "metadata": {
        "id": "sfDs22dHbgVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "S7nTUsq0Uwmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get top N words"
      ],
      "metadata": {
        "id": "NDfPFSxbbizk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_n = 30\n",
        "top_indices = tfidf_means.argsort()[::-1][:top_n]\n",
        "for i in top_indices:\n",
        "    print(f\"{feature_names[i]:<30} {tfidf_means[i]:.6f}\")\n"
      ],
      "metadata": {
        "id": "Jryb1SnAbkmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mIL72Bg5U1wl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get top N words"
      ],
      "metadata": {
        "id": "bh2EE9COU13g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Tokenization, remove stopwords and lemmatize\n",
        "\n",
        "+ WordNetLemmatizer"
      ],
      "metadata": {
        "id": "5hqUH6s9R_vm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Display keywords"
      ],
      "metadata": {
        "id": "pQHZfxW9VcKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame of top terms and their scores\n",
        "import pandas as pd\n",
        "\n",
        "top_keywords = pd.DataFrame({\n",
        "    'term': [feature_names[i] for i in top_indices],\n",
        "    'score': [tfidf_means[i] for i in top_indices]\n",
        "})\n"
      ],
      "metadata": {
        "id": "rjO2kIU36vNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(top_keywords['term'][::-1], top_keywords['score'][::-1])\n",
        "plt.xlabel('Average TF-IDF Score')\n",
        "plt.title(f'Top {top_n} Keywords by TF-IDF')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dvray6SuVd6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "j4zPaHqBU-0q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Heatmap with top N keywords"
      ],
      "metadata": {
        "id": "iyDyuisTWXdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Compute average TF-IDF scores\n",
        "tfidf_means = np.asarray(tfidf_matrix.mean(axis=0)).flatten()\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "tfidf_scores = pd.DataFrame({'term': feature_names, 'score': tfidf_means})\n",
        "\n",
        "# Sort and select top 30\n",
        "top_n = 30\n",
        "top_keywords = tfidf_scores.sort_values(by='score', ascending=False).head(top_n)['term'].tolist()\n",
        "\n",
        "# Convert the sparse matrix to dense and wrap in a DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
        "\n",
        "# Keep only the top 30 keywords\n",
        "tfidf_top_df = tfidf_df[top_keywords]"
      ],
      "metadata": {
        "id": "PLHfQVP1WaFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(tfidf_top_df, cmap='YlGnBu', linewidths=0.5)\n",
        "\n",
        "plt.title(\"TF-IDF Heatmap of Top 30 Keywords Across Reflection Essays\")\n",
        "plt.xlabel(\"Keywords\")\n",
        "plt.ylabel(\"Essays\")\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the plot before showing it\n",
        "plt.savefig(\"tfidf_heatmap.png\", dpi=300)  # You can also use .pdf or .svg\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OW3vQ2CQWf74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "viAgw7b5VrAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Heatmap with top N keywords"
      ],
      "metadata": {
        "id": "xrXZ_yv-VrKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Compute average TF-IDF scores\n",
        "tfidf_means = np.asarray(tfidf_matrix.mean(axis=0)).flatten()\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "tfidf_scores = pd.DataFrame({'term': feature_names, 'score': tfidf_means})\n",
        "\n",
        "# Sort and select top 30\n",
        "top_n = 30\n",
        "top_keywords = tfidf_scores.sort_values(by='score', ascending=False).head(top_n)['term'].tolist()\n",
        "\n",
        "# Convert the sparse matrix to dense and wrap in a DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
        "\n",
        "# Keep only the top 30 keywords\n",
        "tfidf_top_df = tfidf_df[top_keywords]"
      ],
      "metadata": {
        "id": "4jaCmSxDVrKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(tfidf_top_df, cmap='YlGnBu', linewidths=0.5)\n",
        "\n",
        "plt.title(\"TF-IDF Heatmap of Top 30 Keywords Across Reflection Essays\")\n",
        "plt.xlabel(\"Keywords\")\n",
        "plt.ylabel(\"Essays\")\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the plot before showing it\n",
        "plt.savefig(\"tfidf_heatmap.png\", dpi=300)  # You can also use .pdf or .svg\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RvRhCbUOVrKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part II. Group similar keywords into potential themes\n",
        "\n",
        "Identify and label recurring themes across the 33 reflection essays, using a semi-automated method based on high-TF-IDF terms."
      ],
      "metadata": {
        "id": "qXW563VAdXGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ 1. Elbow Method\n",
        "This helps you choose the k (number of clusters) where the gain in performance (inertia) starts to diminish."
      ],
      "metadata": {
        "id": "VA1M-97jEtdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "inertia = []\n",
        "k_range = range(2, 11)\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(tfidf_matrix.T)  # Transpose: terms x documents\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the elbow curve\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(k_range, inertia, marker='o')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Inertia (Within-Cluster Sum of Squares)')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "plt.xticks(k_range)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ROFHm-ixEwoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ 2. Silhouette Score\n",
        "This evaluates how well-separated the clusters are. Higher score = better-defined clusters."
      ],
      "metadata": {
        "id": "s4W3imO8Ez-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "sil_scores = []\n",
        "k_range = range(2, 11)\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(tfidf_matrix.T)\n",
        "    score = silhouette_score(tfidf_matrix.T, labels)\n",
        "    sil_scores.append(score)\n",
        "\n",
        "# Plot silhouette scores\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(k_range, sil_scores, marker='o')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Analysis for Optimal k')\n",
        "plt.xticks(k_range)\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-LrOWriJE2Th"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of themes = 3 or 4 based on the above k values"
      ],
      "metadata": {
        "id": "wviqsANqFmM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Set number of themes (you can adjust after evaluation)\n",
        "num_themes = 4\n",
        "\n",
        "# Transpose to get (terms x documents) shape\n",
        "term_matrix = tfidf_matrix.T\n",
        "\n",
        "# Apply KMeans\n",
        "kmeans = KMeans(n_clusters=num_themes, random_state=42, n_init=10)\n",
        "clusters = kmeans.fit_predict(term_matrix)\n",
        "\n",
        "# Build dataframe with terms and scores\n",
        "tfidf_scores = pd.DataFrame({\n",
        "    'term': feature_names,\n",
        "    'score': tfidf_means,\n",
        "    'cluster': clusters\n",
        "})\n",
        "\n",
        "# Sort for easy inspection\n",
        "tfidf_scores.sort_values(by=['cluster', 'score'], ascending=[True, False], inplace=True)\n",
        "\n",
        "# Display top terms per cluster\n",
        "for c in range(num_themes):\n",
        "    print(f\"\\nCluster {c}:\")\n",
        "    print(tfidf_scores[tfidf_scores['cluster'] == c].head(10)[['term', 'score']])\n"
      ],
      "metadata": {
        "id": "TvmmFUrhEJp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "loBneskKV839"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "k=4"
      ],
      "metadata": {
        "id": "kg686liPJ8mZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot"
      ],
      "metadata": {
        "id": "sWUPKXMdGSjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Reduce the term matrix to 2D for plotting\n",
        "pca = PCA(n_components=2)\n",
        "reduced = pca.fit_transform(term_matrix.toarray())\n",
        "\n",
        "# Plot the terms with cluster coloring\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(reduced[:, 0], reduced[:, 1], c=clusters, cmap='tab10', s=100)\n",
        "\n",
        "# Annotate each point with its term\n",
        "for i, term in enumerate(feature_names):\n",
        "    plt.annotate(term, (reduced[i, 0], reduced[i, 1]), fontsize=9)\n",
        "\n",
        "plt.title(\"TF-IDF Term Clustering (PCA-reduced)\")\n",
        "plt.xlabel(\"PCA 1\")\n",
        "plt.ylabel(\"PCA 2\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"tfidf_clustering_pca.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sF28kG2LGTc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9GYxbSiNWrIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Show only top keywords per cluster"
      ],
      "metadata": {
        "id": "ZRT6Glga97cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show only top N keywords per cluster\n",
        "top_n_per_cluster = 7\n",
        "\n",
        "top_keywords = (\n",
        "    tfidf_scores\n",
        "    .groupby('cluster')\n",
        "    .apply(lambda x: x.sort_values('score', ascending=False).head(top_n_per_cluster))\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# Filter PCA-reduced coordinates to just top keywords\n",
        "selected_indices = [feature_names.tolist().index(term) for term in top_keywords['term']]\n",
        "reduced_selected = reduced[selected_indices]\n",
        "cluster_selected = clusters[selected_indices]\n",
        "terms_selected = top_keywords['term'].tolist()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "scatter = plt.scatter(reduced_selected[:, 0], reduced_selected[:, 1], c=cluster_selected, cmap='tab10', s=100)\n",
        "\n",
        "# Annotate terms\n",
        "for i, term in enumerate(terms_selected):\n",
        "    plt.annotate(term, (reduced_selected[i, 0], reduced_selected[i, 1]), fontsize=10)\n",
        "\n",
        "plt.title(\"TF-IDF Term Clustering (Top Terms Only)\")\n",
        "plt.xlabel(\"PCA 1\")\n",
        "plt.ylabel(\"PCA 2\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1JsGPz9G9-TO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convex hulls to the above PCA plot"
      ],
      "metadata": {
        "id": "EymTc-z1Ckgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import ConvexHull\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Group coordinates, terms, and clusters together\n",
        "points = np.array(reduced_selected)\n",
        "labels = np.array(cluster_selected)\n",
        "terms = np.array(terms_selected)\n",
        "\n",
        "# Plot setup\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = plt.cm.tab10(np.arange(10))\n",
        "\n",
        "# Plot points per cluster with convex hulls\n",
        "for cluster_id in np.unique(labels):\n",
        "    mask = labels == cluster_id\n",
        "    cluster_points = points[mask]\n",
        "\n",
        "    # Plot points\n",
        "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], s=100,\n",
        "                color=colors[cluster_id], label=f\"Theme {cluster_id + 1}\", alpha=0.6)\n",
        "\n",
        "    # Draw convex hull if enough points\n",
        "    if len(cluster_points) >= 3:\n",
        "        hull = ConvexHull(cluster_points)\n",
        "        for simplex in hull.simplices:\n",
        "            plt.plot(cluster_points[simplex, 0], cluster_points[simplex, 1],\n",
        "                     color=colors[cluster_id], linewidth=2)\n",
        "\n",
        "    # Annotate terms\n",
        "    for (x, y), term in zip(cluster_points, terms[mask]):\n",
        "        plt.text(x, y, term, fontsize=10, ha='center', va='center')\n",
        "\n",
        "plt.title(\"TF-IDF Term Clustering (Top Terms Only) with Convex Hulls\")\n",
        "plt.xlabel(\"PCA 1\")\n",
        "plt.ylabel(\"PCA 2\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.text(x, y, term, fontsize=10, ha='center', va='center',\n",
        "         bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))\n",
        "\n",
        "plt.savefig(\"term_clustering_convex.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Y19x5QyPCpgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jkLCRzxhXFzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Avoid overlapping text"
      ],
      "metadata": {
        "id": "QlZmdvnW_aNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install adjustText"
      ],
      "metadata": {
        "id": "sf2buxtq_HSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from adjustText import adjust_text\n",
        "from scipy.spatial import ConvexHull\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Group coordinates, terms, and clusters together\n",
        "points = np.array(reduced_selected)\n",
        "labels = np.array(cluster_selected)\n",
        "terms = np.array(terms_selected)\n",
        "\n",
        "# Plot setup\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = plt.cm.tab10(np.arange(10))\n",
        "texts = []\n",
        "\n",
        "# Plot points per cluster with convex hulls\n",
        "for cluster_id in np.unique(labels):\n",
        "    mask = labels == cluster_id\n",
        "    cluster_points = points[mask]\n",
        "\n",
        "    # Plot points\n",
        "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], s=100,\n",
        "                color=colors[cluster_id], label=f\"Theme {cluster_id + 1}\", alpha=0.6)\n",
        "\n",
        "    # Draw convex hull if enough points\n",
        "    if len(cluster_points) >= 3:\n",
        "        hull = ConvexHull(cluster_points)\n",
        "        for simplex in hull.simplices:\n",
        "            plt.plot(cluster_points[simplex, 0], cluster_points[simplex, 1],\n",
        "                     color=colors[cluster_id], linewidth=2)\n",
        "\n",
        "    # Collect text objects for adjustText\n",
        "    for (x, y), term in zip(cluster_points, terms[mask]):\n",
        "        text = plt.text(x, y, term, fontsize=10, ha='center', va='center')\n",
        "        texts.append(text)\n",
        "\n",
        "# Adjust overlapping texts\n",
        "adjust_text(texts, arrowprops=dict(arrowstyle='-', color='gray', lw=0.5))\n",
        "\n",
        "plt.title(\"TF-IDF Term Clustering (Top Terms Only) with Convex Hulls and Adjusted Labels\")\n",
        "plt.xlabel(\"PCA 1\")\n",
        "plt.ylabel(\"PCA 2\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"term_clustering_convex_good.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5m3rf6BF_yFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Group comparison (trial) before Step2 below"
      ],
      "metadata": {
        "id": "qQu-ASk5aC5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "terms = vectorizer.get_feature_names_out()  # should return 751 terms\n",
        "print(len(terms))  # should now print 751\n"
      ],
      "metadata": {
        "id": "6utyXnKHbJLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Basic pre-processing (stopwords removal is done inside vectorizer)\n",
        "vectorizer = TfidfVectorizer(\n",
        "    stop_words='english',\n",
        "    lowercase=True,\n",
        "    max_df=0.8,\n",
        "    min_df=2,\n",
        "    ngram_range=(1, 2)  # unigrams and bigrams\n",
        ")\n",
        "\n",
        "# Create a new column to ensure no missing values\n",
        "df['Essay-E'] = df['Essay-E'].fillna('')\n",
        "\n",
        "# Fit TF-IDF on all essays\n",
        "tfidf_matrix = vectorizer.fit_transform(df['Essay-E'])\n",
        "\n",
        "# Get feature names\n",
        "terms = vectorizer.get_feature_names_out()\n"
      ],
      "metadata": {
        "id": "Ww4G70VLa1qT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Step 3: Separate TF-IDF matrices by group\n",
        "import numpy as np\n",
        "\n",
        "# Get group indices\n",
        "group_a_idx = df[df['Group'] == 'A'].index\n",
        "group_b_idx = df[df['Group'] == 'B'].index\n",
        "\n",
        "# Subset the TF-IDF matrix\n",
        "tfidf_a = tfidf_matrix[group_a_idx]\n",
        "tfidf_b = tfidf_matrix[group_b_idx]\n",
        "\n",
        "# Compute mean TF-IDF scores per term for each group\n",
        "mean_a = np.asarray(tfidf_a.mean(axis=0)).flatten()\n",
        "mean_b = np.asarray(tfidf_b.mean(axis=0)).flatten()\n"
      ],
      "metadata": {
        "id": "COxuAgrZaQD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Double-check lengths\n",
        "print(len(terms))       # Number of terms in the vocabulary\n",
        "print(len(mean_a))      # Number of TF-IDF features in Group A\n",
        "print(len(mean_b))      # Number of TF-IDF features in Group B\n"
      ],
      "metadata": {
        "id": "5sqcjSyRanvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Step 4: Create a comparison DataFrame\n",
        "comparison_df = pd.DataFrame({\n",
        "    'term': terms,\n",
        "    'mean_A': mean_a,\n",
        "    'mean_B': mean_b,\n",
        "    'diff': mean_a - mean_b\n",
        "})\n",
        "\n",
        "# Sort by absolute difference\n",
        "comparison_df['abs_diff'] = comparison_df['diff'].abs()\n",
        "comparison_df_sorted = comparison_df.sort_values(by='abs_diff', ascending=False)\n"
      ],
      "metadata": {
        "id": "thJdzASYaUpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Step 5: Visualize key differences (optional)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "top_n = 30\n",
        "top_diff = comparison_df_sorted.head(top_n)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(top_diff['term'], top_diff['diff'], color=(top_diff['diff'] > 0).map({True: 'blue', False: 'orange'}))\n",
        "plt.axvline(0, color='gray', linewidth=1)\n",
        "plt.xlabel(\"TF-IDF Difference (A - B)\")\n",
        "plt.title(\"Top Keyword Differences Between Group A and B\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"tfidf_group_comparison.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xNrsyJJnbRVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (Continued codes) - before the group comparison."
      ],
      "metadata": {
        "id": "HGA4zQ95YVIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Step 2: Extract top-N terms by mean TF-IDF\n",
        "top_n = 30\n",
        "top_indices = tfidf_means.argsort()[::-1][:top_n]\n",
        "\n",
        "top_terms = feature_names[top_indices]\n",
        "top_scores = tfidf_means[top_indices]\n",
        "\n",
        "# Create new matrix with only those top terms\n",
        "tfidf_top_matrix = tfidf_matrix[:, top_indices]\n",
        "\n"
      ],
      "metadata": {
        "id": "Lt5eilf0ALJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🎯 Step 3: Cluster the top terms (NOT full terms)\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "num_themes = 5\n",
        "kmeans = KMeans(n_clusters=num_themes, random_state=42)\n",
        "clusters = kmeans.fit_predict(tfidf_top_matrix.T)  # cluster terms\n",
        "\n",
        "# PCA for visualization\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "pca_result = pca.fit_transform(tfidf_top_matrix.T)\n",
        "\n"
      ],
      "metadata": {
        "id": "NMEn6tC9AUu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Step 4: Prepare plotting DataFrame\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "cluster_df = pd.DataFrame({\n",
        "    'term': top_terms,\n",
        "    'score': top_scores,\n",
        "    'cluster': clusters,\n",
        "    'x': pca_result[:, 0],\n",
        "    'y': pca_result[:, 1]\n",
        "})\n",
        "\n"
      ],
      "metadata": {
        "id": "5eENTWzDAZJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "16nrI3bzYgF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🖼️ Step 5: Plot with adjusted text (for overlap)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from adjustText import adjust_text\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
        "\n",
        "for c in range(num_themes):\n",
        "    subset = cluster_df[cluster_df['cluster'] == c]\n",
        "    plt.scatter(subset['x'], subset['y'], label=f\"Theme {c+1}\", s=60, alpha=0.7, color=colors[c])\n",
        "\n",
        "# Add keyword labels\n",
        "texts = []\n",
        "for i, row in cluster_df.iterrows():\n",
        "    texts.append(plt.text(row['x'], row['y'], row['term'], fontsize=10))\n",
        "\n",
        "adjust_text(texts, arrowprops=dict(arrowstyle='-', color='gray'))\n",
        "\n",
        "plt.title(\"Top 30 Keywords Clustered by TF-IDF Similarity (PCA + KMeans)\")\n",
        "plt.xlabel(\"PCA 1\")\n",
        "plt.ylabel(\"PCA 2\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4QptRCt4_iI-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import ConvexHull\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
        "\n",
        "for c in range(num_themes):\n",
        "    subset = cluster_df[cluster_df['cluster'] == c]\n",
        "    plt.scatter(subset['x'], subset['y'], label=f\"Theme {c+1}\", color=colors[c], alpha=0.7)\n",
        "\n",
        "    # Draw convex hull\n",
        "    if len(subset) >= 3:\n",
        "        hull = ConvexHull(subset[['x', 'y']])\n",
        "        for simplex in hull.simplices:\n",
        "            plt.plot(subset['x'].values[simplex], subset['y'].values[simplex], colors[c], linewidth=2)\n",
        "\n",
        "# Label keywords\n",
        "texts = []\n",
        "for _, row in cluster_df.iterrows():\n",
        "    texts.append(plt.text(row['x'], row['y'], row['term'], fontsize=10))\n",
        "adjust_text(texts, arrowprops=dict(arrowstyle='-', color='gray'))\n",
        "\n",
        "plt.title(\"Keyword Clusters with Convex Hulls\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6Qd36J4qCV-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "themes = {}\n",
        "\n",
        "for i in range(num_themes):\n",
        "    terms_in_cluster = tfidf_scores[tfidf_scores['cluster'] == i]\n",
        "    top_terms = terms_in_cluster.sort_values(by='score', ascending=False).head(6)\n",
        "    themes[f\"Theme {i+1}\"] = list(top_terms['term'])\n",
        "\n",
        "# Print results\n",
        "for name, keywords in themes.items():\n",
        "    print(f\"{name}: {', '.join(keywords)}\")\n"
      ],
      "metadata": {
        "id": "sGPtzKdHdfrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "J8unTNtHYwtb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ Next Steps After Theme Extraction"
      ],
      "metadata": {
        "id": "fAsfXaJcTDlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Interpret and Label Each Theme"
      ],
      "metadata": {
        "id": "686NspWnkn2d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (name, keywords) in enumerate(themes.items(), 1):\n",
        "    print(f\"Theme {i}: {', '.join(keywords)}\")\n",
        "    # After printing, manually add:\n",
        "    # → Suggested Label: e.g., \"Digital Confidence and Skill Growth\"\n"
      ],
      "metadata": {
        "id": "XcxMq3uvkpoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Visualize Theme-Keyword Relationships"
      ],
      "metadata": {
        "id": "lLahv9cmksEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add edges between theme and its keywords\n",
        "for theme, keywords in themes.items():\n",
        "    for kw in keywords:\n",
        "        G.add_edge(theme, kw)\n",
        "\n",
        "# Plot the network\n",
        "plt.figure(figsize=(10, 6))\n",
        "pos = nx.spring_layout(G, seed=42)\n",
        "nx.draw(G, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=1500, font_size=10)\n",
        "plt.title(\"Thematic Keyword Network\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fLQKZZfQkuEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tTGkWajwY9C-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "GMM trials (7/27)"
      ],
      "metadata": {
        "id": "IYAD8v2mhjVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Install required libraries\n",
        "!pip install matplotlib networkx scikit-learn pandas --quiet"
      ],
      "metadata": {
        "id": "JveDP2QhiFNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from sklearn.covariance import GraphicalLassoCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# STEP 3: Upload your data\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# STEP 4: Load your CSV file\n",
        "df = pd.read_csv(list(uploaded.keys())[0])\n",
        "\n",
        "# STEP 5: Select relevant columns by name or index\n",
        "# Replace with your actual column names if different\n",
        "teaching_cols = ['Q1', 'Q4']\n",
        "learning_cols = ['Q2','Q5']\n",
        "tech_cols = ['Q3','Q6']\n",
        "\n",
        "selected_cols = teaching_cols + learning_cols + tech_cols\n",
        "data = df[selected_cols].dropna()\n",
        "\n",
        "# STEP 6: Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(data)\n",
        "\n",
        "# STEP 7: Estimate GGM using Graphical Lasso\n",
        "model = GraphicalLassoCV()\n",
        "model.fit(X)\n",
        "\n",
        "# STEP 8: Build graph from precision matrix\n",
        "precision = model.precision_\n",
        "partial_corr = -precision / np.outer(np.sqrt(np.diag(precision)), np.sqrt(np.diag(precision)))\n",
        "np.fill_diagonal(partial_corr, 0)\n",
        "\n",
        "# Threshold to remove very weak edges\n",
        "threshold = 0.1\n",
        "adjacency = (np.abs(partial_corr) > threshold).astype(int)\n",
        "\n",
        "# Create graph\n",
        "G = nx.Graph()\n",
        "labels = selected_cols\n",
        "\n",
        "# Add nodes with color group\n",
        "for i, label in enumerate(labels):\n",
        "    if label in teaching_cols:\n",
        "        G.add_node(label, group='Teaching')\n",
        "    elif label in learning_cols:\n",
        "        G.add_node(label, group='Learning')\n",
        "    else:\n",
        "        G.add_node(label, group='Tech')\n",
        "\n",
        "# Add edges with weight (partial correlation)\n",
        "for i in range(len(labels)):\n",
        "    for j in range(i + 1, len(labels)):\n",
        "        if adjacency[i, j]:\n",
        "            G.add_edge(labels[i], labels[j], weight=partial_corr[i, j])\n",
        "\n",
        "# STEP 9: Visualize with colored groups\n",
        "plt.figure(figsize=(10, 8))\n",
        "pos = nx.spring_layout(G, seed=42)\n",
        "\n",
        "# Colors for groups\n",
        "group_colors = {\n",
        "    'Teaching': '#66c2a5',\n",
        "    'Learning': '#fc8d62',\n",
        "    'Tech': '#8da0cb'\n",
        "}\n",
        "node_colors = [group_colors[G.nodes[node]['group']] for node in G.nodes]\n",
        "\n",
        "# Draw\n",
        "nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=800, alpha=0.9)\n",
        "nx.draw_networkx_labels(G, pos, font_size=10)\n",
        "nx.draw_networkx_edges(G, pos, edge_color='gray', width=1.5, alpha=0.7)\n",
        "\n",
        "plt.title(\"GGM Network of ChatGPT Survey Beliefs\", fontsize=14)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dn63LeBFhnYq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}