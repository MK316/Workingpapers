{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPv/pv9I3NhgSkXRaZIAw8n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MK316/Workingpapers/blob/main/Selfefficacy/self_data01_0807.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-efficacy analysis (24.08.07)"
      ],
      "metadata": {
        "id": "wIdJtJDCvZZW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWGP-x4jsFPb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace 'path_to_file.csv' with the actual path of your CSV file\n",
        "path_to_file = 'self-data01.csv'\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv(path_to_file)\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming df is your DataFrame and TOEIC is the column you're analyzing\n",
        "mean_toeic = df['TOEIC'].mean()\n",
        "std_toeic = df['TOEIC'].std()\n",
        "\n",
        "print(\"Mean of TOEIC scores:\", mean_toeic)\n",
        "print(\"Standard Deviation of TOEIC scores:\", std_toeic)\n"
      ],
      "metadata": {
        "id": "XgjK005Nv1MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1 analysis"
      ],
      "metadata": {
        "id": "0cXLglaFwnfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "# Calculate descriptive statistics for Q1\n",
        "descriptive_stats_q1 = df['Q1'].describe()\n",
        "print(\"Descriptive statistics for Q1:\")\n",
        "print(descriptive_stats_q1)\n",
        "\n",
        "# Create a visual summary of the results for Q1\n",
        "# Assuming the responses for Q1 are on a Likert scale (e.g., 1 to 5)\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x='Q1', data=df, palette='Blues')\n",
        "plt.title('Distribution of Responses for Q1')\n",
        "plt.xlabel('Response Category')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)  # Add grid for better readability\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig('Q1-figure.png')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lQLXh4ZlwpGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "17/20"
      ],
      "metadata": {
        "id": "d7wPGdrzDDDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load your data into a DataFrame\n",
        "# df = pd.read_csv('path_to_your_file.csv')  # Uncomment and set your file path\n",
        "\n",
        "# Assuming Q1 responses are categorical (e.g., on a Likert scale) and Career is also categorical\n",
        "# Create a crosstabulation of the data\n",
        "ct = pd.crosstab(df['Career'], df['Q1'])\n",
        "\n",
        "# Normalize the crosstab to show percentages\n",
        "ct_normalized = ct.div(ct.sum(axis=1), axis=0)\n",
        "\n",
        "# Plotting the crosstab\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.heatmap(ct_normalized, annot=True, cmap='Blues', fmt='.2f')\n",
        "plt.title('Relationship Between Q1 Responses and Career')\n",
        "plt.xlabel('Q1 Responses')\n",
        "plt.ylabel('Career')\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig('Q1b-figure.png')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BYgelaDXyVZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "# Load your data into a DataFrame if needed\n",
        "# df = pd.read_csv('path_to_your_file.csv')  # Uncomment and set your file path\n",
        "\n",
        "# Count the frequency of each response in Q1 and sort by index for response order\n",
        "frequency = df['Q1'].value_counts().sort_index()\n",
        "\n",
        "# Calculate the cumulative percentage\n",
        "cumulative_percentage = frequency.cumsum() / frequency.sum() * 100\n",
        "\n",
        "# Create a figure and a bar plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "bars = ax.bar(frequency.index, frequency.values, color='skyblue')\n",
        "\n",
        "# Add a line plot for the cumulative percentage\n",
        "ax2 = ax.twinx()\n",
        "line = ax2.plot(frequency.index, cumulative_percentage, color='red', marker='o', label='Cumulative %')\n",
        "\n",
        "# Make the y-axis label, ticks, and tick labels match the line plot\n",
        "ax2.set_ylabel('Cumulative Percentage')\n",
        "ax2.set_ylim(0, 110)\n",
        "\n",
        "# Adding annotations and labels\n",
        "ax.set_xlabel('Q1 Responses')\n",
        "ax.set_ylabel('Frequency')\n",
        "ax.set_title('Pareto Chart for Q1 Responses')\n",
        "ax2.legend(loc='right')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1OfFZsCU2krw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2 analysis"
      ],
      "metadata": {
        "id": "0QEeRF0pRs6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "# Calculate descriptive statistics for Q1\n",
        "descriptive_stats_q1 = df['Q2'].describe()\n",
        "print(\"Descriptive statistics for Q2:\")\n",
        "print(descriptive_stats_q1)\n",
        "\n",
        "# Create a visual summary of the results for Q1\n",
        "# Assuming the responses for Q1 are on a Likert scale (e.g., 1 to 5)\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x='Q2', data=df, palette='Blues')\n",
        "plt.title('Distribution of Responses for Q2')\n",
        "plt.xlabel('Response Category')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)  # Add grid for better readability\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig('Q2-figure.png')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Uvl8yJvhRvhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3 analysis: Anxiety"
      ],
      "metadata": {
        "id": "plXjDaGDTojp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "# Calculate descriptive statistics for Q1\n",
        "descriptive_stats_q1 = df['Q3'].describe()\n",
        "print(\"Descriptive statistics for Q3:\")\n",
        "print(descriptive_stats_q1)\n",
        "\n",
        "# Create a visual summary of the results for Q1\n",
        "# Assuming the responses for Q1 are on a Likert scale (e.g., 1 to 5)\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.countplot(x='Q3', data=df, palette='Blues')\n",
        "plt.title('Distribution of Responses for Q3')\n",
        "plt.xlabel('Response Category')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True, linestyle='--', alpha=0.6)  # Add grid for better readability\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig('Q3-figure.png')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wkzUwVR9SANB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4 & 5"
      ],
      "metadata": {
        "id": "KA0aY-s9WHkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming df is your DataFrame and it contains 'Q4' and 'Q5' columns for pre- and post-training responses\n",
        "# Create a paired bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=df[['Q4', 'Q5']], ci=None)\n",
        "plt.title('Comparison of Confidence Levels Before and After Training')\n",
        "plt.ylabel('Confidence Level')\n",
        "plt.xticks([0, 1], ['Before Training', 'After Training'])\n",
        "plt.show()\n",
        "\n",
        "# Create a box plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(data=df[['Q4', 'Q5']])\n",
        "plt.title('Box Plot of Confidence Levels Before and After Training')\n",
        "plt.ylabel('Confidence Score')\n",
        "plt.xticks([0, 1], ['Before Training', 'After Training'])\n",
        "\n",
        "plt.savefig(\"Q45-boxplot.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "XcWH3jSTWJGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import wilcoxon\n",
        "\n",
        "# Load your data\n",
        "# Make sure your DataFrame 'df' has the columns 'Q4' and 'Q5' representing the pre- and post-training scores\n",
        "# df = pd.read_csv('path_to_your_file.csv')\n",
        "\n",
        "\n",
        "# Conducting the Wilcoxon Signed-Rank Test\n",
        "stat, p_value = wilcoxon(df['Q4'], df['Q5'])\n",
        "\n",
        "# Printing the statistics\n",
        "print(f'Wilcoxon Signed-Rank Test Statistic: {stat}')\n",
        "print(f'P-Value: {p_value}')\n",
        "\n",
        "# Visualization of the results\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.distplot(df['Q4'], label='Before Training', kde=False, norm_hist=True)\n",
        "sns.distplot(df['Q5'], label='After Training', kde=False, norm_hist=True)\n",
        "plt.title('Confidence Level Before and After Training')\n",
        "plt.xlabel('Confidence Score')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# You might want to add annotations or additional details to visualize the p-value\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.bar(['Before Training', 'After Training'], [df['Q4'].mean(), df['Q5'].mean()], yerr=[df['Q4'].std(), df['Q5'].std()])\n",
        "plt.title('Mean Confidence Scores Before and After Training')\n",
        "plt.ylabel('Average Confidence Score')\n",
        "plt.text(1, df['Q5'].mean(), f'p = {p_value:.3f}', ha='center', va='bottom')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3deZHf_ZWdRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changes of responses in Q4 and Q5"
      ],
      "metadata": {
        "id": "t2HKQ6SXXz4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your data\n",
        "# df = pd.read_csv('path_to_your_file.csv')\n",
        "\n",
        "# Calculate changes\n",
        "changes = df['Q5'] - df['Q4']\n",
        "\n",
        "# Count increases, decreases, and no changes\n",
        "increased = (changes > 0).sum()\n",
        "decreased = (changes < 0).sum()\n",
        "no_change = (changes == 0).sum()\n",
        "\n",
        "# Print results\n",
        "print(f\"Number of responses that increased: {increased}\")\n",
        "print(f\"Number of responses that decreased: {decreased}\")\n",
        "print(f\"Number of responses with no change: {no_change}\")\n"
      ],
      "metadata": {
        "id": "PY6ku009X3kO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate descriptive statistics for Q1\n",
        "descriptive_stats_q1 = df['Q4'].describe()\n",
        "print(\"Descriptive statistics for Q4:\")\n",
        "print(descriptive_stats_q1)"
      ],
      "metadata": {
        "id": "fa9xNajiY3R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "descriptive_stats_q1 = df['Q5'].describe()\n",
        "print(\"Descriptive statistics for Q5:\")\n",
        "print(descriptive_stats_q1)"
      ],
      "metadata": {
        "id": "vH4mr8D_Y6RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "16/20"
      ],
      "metadata": {
        "id": "fcRRAvF9evSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part II. Perceptions (Q6~Q10)\n",
        "\n",
        "The order changed (1, 2 => removed; 3, 4, 5, 6, 7) => 3, 6, 4, 5, 7\n",
        "\n",
        "+ These become Q6~Q10 in self-data02.csv (asof 0908)"
      ],
      "metadata": {
        "id": "biswHIjMg2Ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace 'path_to_file.csv' with the actual path of your CSV file\n",
        "path_to_file = 'self-data02.csv'\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv(path_to_file)\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "7PDp9Ktbohnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Assuming df is your DataFrame and it contains 'Q6' and 'Q7' columns for the respective responses\n",
        "# Load your data - Uncomment and modify the next line if your DataFrame is not yet loaded\n",
        "# df = pd.read_csv('path_to_your_file.csv')\n",
        "\n",
        "# Descriptive statistics\n",
        "print(\"Descriptive Statistics for Q6:\")\n",
        "print(df['Q6'].describe())\n",
        "print(\"\\nDescriptive Statistics for Q7:\")\n",
        "print(df['Q7'].describe())\n",
        "\n",
        "# Spearman correlation to check the relationship between Q6 and Q7\n",
        "correlation, p_value = spearmanr(df['Q6'], df['Q7'])\n",
        "print(f\"\\nSpearman Correlation between Q6 and Q7: {correlation:.2f}, P-value: {p_value:.4f}\")\n",
        "\n",
        "# Visualization of responses\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.boxplot(y=df['Q6'])\n",
        "plt.title('Boxplot of Responses for Q6')\n",
        "plt.ylabel('Ratings for Q6')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(y=df['Q7'])\n",
        "plt.title('Boxplot of Responses for Q7')\n",
        "plt.ylabel('Ratings for Q7')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Optionally, visualize both responses in a joint distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.jointplot(x=df['Q6'], y=df['Q7'], kind='scatter')\n",
        "plt.xlabel('Q6 Ratings')\n",
        "plt.ylabel('Q7 Ratings')\n",
        "plt.xlim(1,11)\n",
        "plt.ylim(1,11)\n",
        "plt.title('Joint Distribution of Q6 and Q7 Responses', pad=80)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "sn_8gHyQo4GD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "fixing the edges"
      ],
      "metadata": {
        "id": "dPuhX61frzRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Assuming df is your DataFrame and it contains 'Q6' and 'Q7' columns for the respective responses\n",
        "# Load your data - Uncomment and modify the next line if your DataFrame is not yet loaded\n",
        "# df = pd.read_csv('path_to_your_file.csv')\n",
        "\n",
        "# Descriptive statistics\n",
        "print(\"Descriptive Statistics for Q6:\")\n",
        "print(df['Q6'].describe())\n",
        "print(\"\\nDescriptive Statistics for Q7:\")\n",
        "print(df['Q7'].describe())\n",
        "\n",
        "# Spearman correlation to check the relationship between Q6 and Q7\n",
        "correlation, p_value = spearmanr(df['Q6'], df['Q7'])\n",
        "print(f\"\\nSpearman Correlation between Q6 and Q7: {correlation:.2f}, P-value: {p_value:.4f}\")\n",
        "\n",
        "# Visualization of responses\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.boxplot(y=df['Q6'])\n",
        "plt.title('Boxplot of Responses for Q6')\n",
        "plt.ylabel('Ratings for Q6')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(y=df['Q7'])\n",
        "plt.title('Boxplot of Responses for Q7')\n",
        "plt.ylabel('Ratings for Q7')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Optionally, visualize both responses in a joint distribution\n",
        "# Adjust the xlim and ylim based on the actual data range with a small padding\n",
        "g = sns.jointplot(x=df['Q6'], y=df['Q7'], kind='scatter', s=100, edgecolor=\"w\", linewidth=1)\n",
        "g.fig.suptitle('Joint Distribution of Q6 and Q7 Responses', y=1.02) # Adjust title location\n",
        "g.set_axis_labels('Q6 Ratings', 'Q7 Ratings')\n",
        "g.ax_joint.set_xlim(df['Q6'].min() - 0.5, df['Q6'].max() + 0.5)\n",
        "g.ax_joint.set_ylim(df['Q7'].min() - 0.5, df['Q7'].max() + 0.5)\n",
        "plt.savefig(\"Q67-jointplot.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Qwu4ebkcryV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming your DataFrame df is already loaded\n",
        "# Example DataFrame (comment out if your DataFrame is ready)\n",
        "# data = {'Q6': [6, 7, 8, 9, 10], 'Q7': [6, 7, 8, 9, 10]}\n",
        "# df = pd.DataFrame(data)\n",
        "\n",
        "# Optionally, visualize both responses in a joint distribution\n",
        "g = sns.jointplot(x='Q6', y='Q7', data=df, kind='scatter', s=100, edgecolor=\"w\", linewidth=1)\n",
        "\n",
        "# Adjust xlim and ylim to ensure all data points are entirely visible within the plot\n",
        "g.ax_joint.set_xlim(1, 10.5)  # Setting the x-axis limits from 1 to 10.5\n",
        "g.ax_joint.set_ylim(1, 10.5)  # Setting the y-axis limits from 1 to 10.5\n",
        "\n",
        "# Enhance the title and labels\n",
        "g.fig.suptitle('Joint Distribution of Q6 and Q7 Responses', y=1.02)\n",
        "g.set_axis_labels('Q6 Ratings', 'Q7 Ratings')\n",
        "\n",
        "plt.savefig(\"Q67-jointplot.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ltsg_rJ1s6wG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming df is your DataFrame and it contains 'Q8' and 'Q9' columns for the respective responses\n",
        "# Load your data - Uncomment and modify the next line if your DataFrame is not yet loaded\n",
        "# df = pd.read_csv('path_to_your_file.csv')\n",
        "\n",
        "# Create a DataFrame suitable for seaborn's boxplot\n",
        "data_to_plot = pd.melt(df, id_vars=[], value_vars=['Q6', 'Q7'], var_name='Question', value_name='Rating')\n",
        "\n",
        "# Plotting combined boxplots with specific colors\n",
        "plt.figure(figsize=(8, 6))\n",
        "palette = {\"Q6\": \"white\", \"Q7\": \"#1f77b4\"}  # Define a color palette, #1f77b4 is the default matplotlib blue\n",
        "sns.boxplot(x='Question', y='Rating', data=data_to_plot, palette=palette)\n",
        "plt.title('Combined Boxplot of Responses for Q6 and Q7')\n",
        "plt.xlabel('Question')\n",
        "plt.ylabel('Ratings')\n",
        "plt.ylim(1,10.5)\n",
        "\n",
        "plt.savefig(\"Q67combined-boxplot.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "O2WfbzADwJOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q8 & 9"
      ],
      "metadata": {
        "id": "BmaT7ag4u3YI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "# Assuming df is your DataFrame and it contains 'Q8' and 'Q9' columns for the respective responses\n",
        "# Load your data - Uncomment and modify the next line if your DataFrame is not yet loaded\n",
        "# df = pd.read_csv('path_to_your_file.csv')\n",
        "\n",
        "# Calculate descriptive statistics for both questions\n",
        "print(\"Descriptive Statistics for Q8:\")\n",
        "print(df['Q8'].describe())\n",
        "print(\"\\nDescriptive Statistics for Q9:\")\n",
        "print(df['Q9'].describe())\n",
        "\n",
        "# Calculate Spearman correlation to check the relationship between Q8 and Q9\n",
        "correlation, p_value = spearmanr(df['Q8'], df['Q9'])\n",
        "print(f\"\\nSpearman Correlation between Q8 and Q9: {correlation:.2f}, P-value: {p_value:.4f}\")\n",
        "\n",
        "# Visualization of responses\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.boxplot(y=df['Q8'])\n",
        "plt.title('Boxplot of Responses for Q8')\n",
        "plt.ylabel('Incorporation of Pronunciation Strategies')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(y=df['Q9'])\n",
        "plt.title('Boxplot of Responses for Q9')\n",
        "plt.ylabel('Improvement in Pronunciation Skills')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Optionally, visualize both responses in a joint distribution to observe the correlation visually\n",
        "g = sns.jointplot(x=df['Q8'], y=df['Q9'], kind='scatter')\n",
        "\n",
        "# Adjust xlim and ylim to ensure all data points are entirely visible within the plot\n",
        "g.ax_joint.set_xlim(1, 10.5)  # Setting the x-axis limits from 1 to 10.5\n",
        "g.ax_joint.set_ylim(1, 10.5)  # Setting the y-axis limits from 1 to 10.5\n",
        "\n",
        "g.fig.suptitle('Joint Distribution of Q8 and Q9 Responses', y=1.02)\n",
        "g.set_axis_labels('Q8 Ratings', 'Q9 Ratings')\n",
        "plt.savefig(\"Q89-jointplot.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "otVcrTDRu5fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qaTj2AYYwHQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming df is your DataFrame and it contains 'Q8' and 'Q9' columns for the respective responses\n",
        "# Load your data - Uncomment and modify the next line if your DataFrame is not yet loaded\n",
        "# df = pd.read_csv('path_to_your_file.csv')\n",
        "\n",
        "# Create a DataFrame suitable for seaborn's boxplot\n",
        "data_to_plot = pd.melt(df, id_vars=[], value_vars=['Q8', 'Q9'], var_name='Question', value_name='Rating')\n",
        "\n",
        "# Plotting combined boxplots with specific colors\n",
        "plt.figure(figsize=(8, 6))\n",
        "palette = {\"Q8\": \"white\", \"Q9\": \"#1f77b4\"}  # Define a color palette, #1f77b4 is the default matplotlib blue\n",
        "sns.boxplot(x='Question', y='Rating', data=data_to_plot, palette=palette)\n",
        "plt.title('Combined Boxplot of Responses for Q8 and Q9')\n",
        "plt.xlabel('Question')\n",
        "plt.ylabel('Ratings')\n",
        "\n",
        "plt.savefig(\"Q89-boxplot.png\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6u67lJh1vRgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q10"
      ],
      "metadata": {
        "id": "q7E8ON_s1vCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming df is your DataFrame and it contains 'Q10' column for the respective responses\n",
        "# Load your data - Uncomment and modify the next line if your DataFrame is not yet loaded\n",
        "# df = pd.read_csv('path_to_your_file.csv')\n",
        "\n",
        "# Plotting a histogram of Q10 responses\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.histplot(df['Q10'], bins=5, kde=False, color='skyblue')\n",
        "plt.title('Histogram of Recommendations for Pronunciation Training')\n",
        "plt.xlabel('Recommendation Level')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(range(1, 11))  # Assuming the recommendation levels are rated from 1 to 10\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plotting a pie chart of Q10 responses\n",
        "# Counting the number of each response level\n",
        "recommendation_counts = df['Q10'].value_counts()\n",
        "\n",
        "# Creating the pie chart\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(recommendation_counts, labels=recommendation_counts.index, autopct='%1.1f%%', startangle=140, colors=sns.color_palette(\"Set2\"))\n",
        "plt.title('Pie Chart of Recommendations for Pronunciation Training')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "usOWaXIj1wOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCA"
      ],
      "metadata": {
        "id": "Rwh2DIUx4-2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load your data\n",
        "df = pd.read_csv('self-data03.csv')  # Assume data is already preprocessed\n",
        "\n",
        "# Standardizing the data is often recommended for PCA\n",
        "scaler = StandardScaler()\n",
        "df_scaled = scaler.fit_transform(df)\n",
        "\n",
        "# Creating PCA object and fitting to scaled data\n",
        "pca = PCA(n_components=2)  # Adjust components based on the scree plot analysis\n",
        "pca_results = pca.fit_transform(df_scaled)\n",
        "\n",
        "# Explained variance can help understand the importance of each component\n",
        "print(pca.explained_variance_ratio_)\n",
        "\n",
        "# Loading scores for each principal component\n",
        "print(pca.components_)\n",
        "\n",
        "# Convert components to a DataFrame for better readability\n",
        "components_df = pd.DataFrame(pca.components_, columns=df.columns, index=['Component 1', 'Component 2'])\n",
        "print(components_df)\n"
      ],
      "metadata": {
        "id": "VJHn6Zpr4_5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# PCA components and explained variance (for the example you gave)\n",
        "explained_variance = np.array([0.4065365, 0.16165313])\n",
        "components = np.array([\n",
        "    [-0.29052721, -0.09154982, -0.31252354, -0.3106881, -0.26698935, -0.0459496, -0.18940246, -0.24814029, -0.2580659, -0.37847773, -0.32297778, -0.34890135, -0.32601033],\n",
        "    [-0.3270757, -0.43020617, 0.12095924, -0.21197667, 0.11595744, 0.43564987, -0.33828528, -0.21206049, 0.40324174, 0.16750836, -0.16909148, 0.11486756, 0.23083746]\n",
        "])\n",
        "\n",
        "# Names of the variables\n",
        "variables = ['Career', 'TOEIC', 'Selfeval', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', 'Q7', 'Q8', 'Q9', 'Q10']\n",
        "\n",
        "# Create a biplot\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i, var_names in enumerate(variables):\n",
        "    plt.arrow(0, 0, components[0, i], components[1, i], color='r', alpha=0.5)\n",
        "    plt.text(components[0, i], components[1, i], var_names, color='black', ha='center', va='center')\n",
        "\n",
        "# Scale x and y axis by explained variance (optional, but provides scaling based on variance explained by PCs)\n",
        "plt.xlim(-1, 1)\n",
        "plt.ylim(-1, 1)\n",
        "plt.xlabel(f'PC1 - {explained_variance[0]:.2f}%')\n",
        "plt.ylabel(f'PC2 - {explained_variance[1]:.2f}%')\n",
        "plt.title('Biplot of PCA')\n",
        "plt.grid(True)\n",
        "plt.axhline(0, color='grey', lw=1)\n",
        "plt.axvline(0, color='grey', lw=1)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "PeyhWIg-5jLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-reflection essay: sentiment analysis"
      ],
      "metadata": {
        "id": "qK5mM924HJGN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# After uploading, you can read the file\n",
        "# Assume the uploaded file is named 'your_file.csv'. You can change this to your actual file name.\n",
        "df = pd.read_csv('self-reflection-essay.csv', encoding='utf-8')  # Use 'utf-8' encoding to handle Korean texts\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "df.head()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hKfDxZe3HMxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Missing data to remove (N=15)"
      ],
      "metadata": {
        "id": "OPv1FbJjIhVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Filter out rows where 'SRE1' column has the value 'Missing'\n",
        "data = df[df['SRE1'] != 'Missing']\n",
        "\n",
        "# Step 2: Display the filtered DataFrame\n",
        "data.head()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AnaU1zIwHkpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install the necessary libraries\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "9gi4uWolI35H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install the necessary libraries\n",
        "!pip install transformers\n",
        "\n",
        "# Step 2: Import the necessary libraries\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "# Step 6: Initialize the pre-trained sentiment analysis pipeline\n",
        "sentiment_analyzer = pipeline('sentiment-analysis')\n",
        "\n",
        "# Step 7: Analyze the sentiment of the texts in the 'SRE1' column\n",
        "sentiments = data['SRE1'].apply(lambda text: sentiment_analyzer(text)[0])\n",
        "\n",
        "# Step 8: Add the sentiment results to the DataFrame\n",
        "data['Sentiment'] = sentiments.apply(lambda x: x['label'])\n",
        "data['Sentiment Score'] = sentiments.apply(lambda x: x['score'])\n",
        "\n",
        "# Step 9: Display the results\n",
        "data[['SRE1', 'Sentiment', 'Sentiment Score']].head()\n"
      ],
      "metadata": {
        "id": "7XbyppyPH_GW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seaborn"
      ],
      "metadata": {
        "id": "LGSi0WlsI1VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Step 1: Plot sentiment distribution (Bar chart)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(data=data, x='Sentiment')\n",
        "plt.title('Sentiment Distribution')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n",
        "\n",
        "# Step 2: Plot sentiment score distribution (Histogram)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(data['Sentiment Score'], bins=10, kde=True)\n",
        "plt.title('Sentiment Score Distribution')\n",
        "plt.xlabel('Sentiment Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Step 3: Plot sentiment score distribution (Boxplot for more detailed distribution view)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(data=data, x='Sentiment', y='Sentiment Score')\n",
        "plt.title('Sentiment Score by Sentiment Category')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Sentiment Score')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3clC-icvJR6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretion: In the histogram showing Sentiment Score Distribution, we observe how sentiment scores are spread across the dataset. Here's a breakdown of how to interpret the result:\n",
        "\n",
        "Concentration of High Scores:\n",
        "\n",
        "The highest frequency of sentiment scores appears between 0.95 and 1.00, indicating that the majority of sentiment predictions were made with high confidence (close to 100%). This suggests that the sentiment analysis model was quite confident about the sentiments it predicted for most of the texts.\n",
        "Distribution Tail:\n",
        "\n",
        "A few instances are scattered across lower scores, specifically in the ranges around 0.75 to 0.85. These indicate cases where the model's confidence in its predictions was slightly lower, meaning the model was less certain about the sentiment for those specific texts.\n",
        "Shape of the Distribution:\n",
        "\n",
        "The shape of the distribution shows a peak near 1.00, with a smaller number of observations between 0.75 and 0.85. This suggests that most of the sentiment classifications were made with a high degree of certainty, while only a few instances had less certain predictions.\n",
        "Key Insight:\n",
        "The model generally produced sentiment predictions with high confidence (closer to 1.00), meaning that most of the text analyzed had sentiments that were clearly identifiable by the model. However, the lower scores might represent more ambiguous texts, where the sentiment was harder for the model to classify."
      ],
      "metadata": {
        "id": "MsQ9uCK8KI7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code to filter and display texts with lower confidence:"
      ],
      "metadata": {
        "id": "L03xb1ilKPGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a threshold for low confidence scores\n",
        "threshold = 0.85\n",
        "\n",
        "# Filter the DataFrame for rows where the sentiment score is below the threshold\n",
        "low_confidence_texts = data[data['Sentiment Score'] < threshold]\n",
        "\n",
        "# Display the texts with low sentiment scores\n",
        "low_confidence_texts[['SRE1', 'Sentiment', 'Sentiment Score']]\n"
      ],
      "metadata": {
        "id": "6iMK95gdKTF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VADER\n",
        "\n",
        "To analyze texts using sentiment compound scores, we can employ a tool like VADER (Valence Aware Dictionary and sEntiment Reasoner), which is specifically designed for sentiment analysis in social media text but works well with other forms of text as well. VADER assigns each text a compound score, which ranges from -1 (extremely negative) to +1 (extremely positive), providing a quantitative measure of sentiment intensity."
      ],
      "metadata": {
        "id": "GQIsGA0qK9Zx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install and import necessary libraries\n",
        "!pip install nltk"
      ],
      "metadata": {
        "id": "-U9FOJW6LJCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import pandas as pd\n",
        "\n",
        "# Download VADER lexicon (required for first use)\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Step 2: Initialize the VADER sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Step 3: Create a function to get the compound score\n",
        "def get_compound_score(text):\n",
        "    return sia.polarity_scores(text)['compound']\n",
        "\n",
        "# Step 4: Apply VADER sentiment analysis to the 'SRE1' column\n",
        "data['Compound Score'] = data['SRE1'].apply(get_compound_score)\n",
        "\n",
        "# Step 5: Display the first few rows to check results\n",
        "data[['SRE1', 'Compound Score']].head()\n"
      ],
      "metadata": {
        "id": "qrArddgmK82x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting compound scores for SRE1, SRE2, SRE3"
      ],
      "metadata": {
        "id": "Y_72izzbbnJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import pandas as pd\n",
        "\n",
        "# Download VADER lexicon (required for first use)\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Step 2: Initialize the VADER sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Step 3: Create a function to get the compound score\n",
        "def get_compound_score(text):\n",
        "    return sia.polarity_scores(text)['compound']\n",
        "\n",
        "# Step 4: Apply VADER sentiment analysis to the 'SRE1', 'SRE2', and 'SRE3' columns\n",
        "data['Compound_Score_SRE1'] = data['SRE1'].apply(get_compound_score)\n",
        "data['Compound_Score_SRE2'] = data['SRE2'].apply(get_compound_score)\n",
        "data['Compound_Score_SRE3'] = data['SRE3'].apply(get_compound_score)\n",
        "\n",
        "# Step 5: Display the first few rows to check results\n",
        "data[['SRE1', 'Compound_Score_SRE1', 'SRE2', 'Compound_Score_SRE2', 'SRE3', 'Compound_Score_SRE3']].head()\n"
      ],
      "metadata": {
        "id": "gi3TIMvdbh62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 6: Plot the distribution of compound scores\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(data['Compound Score'], bins=20, kde=True)\n",
        "plt.title('Distribution of Compound Sentiment Scores')\n",
        "plt.xlabel('Compound Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "H2AERgioLMt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter and display rows with negative compound scores\n",
        "negative_sentiment = data[data['Compound Score'] < 0]\n",
        "\n",
        "# Show the negative sentiment rows\n",
        "negative_sentiment[['SRE1', 'Compound Score']]\n"
      ],
      "metadata": {
        "id": "MDgU_A-DMN8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Averaged sentiment scores: SRE1, SRE2, SRE3"
      ],
      "metadata": {
        "id": "cdTVTXKDMiJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install necessary libraries (uncomment if needed)\n",
        "# !pip install nltk\n",
        "\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import pandas as pd\n",
        "\n",
        "# Download VADER lexicon\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Step 2: Initialize VADER sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Step 3: Create a function to get the full sentiment scores (neg, neu, pos, compound)\n",
        "def get_sentiment_scores(text):\n",
        "    return sia.polarity_scores(text)\n",
        "\n",
        "# Step 4: Apply VADER sentiment analysis to 'SRE1', 'SRE2', and 'SRE3' columns and store the results\n",
        "sentiment_scores_sre1 = data['SRE1'].apply(get_sentiment_scores)\n",
        "sentiment_scores_sre2 = data['SRE2'].apply(get_sentiment_scores)\n",
        "sentiment_scores_sre3 = data['SRE3'].apply(get_sentiment_scores)\n",
        "\n",
        "# Convert the sentiment scores into separate DataFrames for each column\n",
        "sentiment_df_sre1 = pd.DataFrame(sentiment_scores_sre1.tolist())\n",
        "sentiment_df_sre2 = pd.DataFrame(sentiment_scores_sre2.tolist())\n",
        "sentiment_df_sre3 = pd.DataFrame(sentiment_scores_sre3.tolist())\n",
        "\n",
        "# Step 5: Calculate average neg, neu, pos scores for each sentiment\n",
        "avg_neg_sre1 = sentiment_df_sre1['neg'].mean()\n",
        "avg_neu_sre1 = sentiment_df_sre1['neu'].mean()\n",
        "avg_pos_sre1 = sentiment_df_sre1['pos'].mean()\n",
        "\n",
        "avg_neg_sre2 = sentiment_df_sre2['neg'].mean()\n",
        "avg_neu_sre2 = sentiment_df_sre2['neu'].mean()\n",
        "avg_pos_sre2 = sentiment_df_sre2['pos'].mean()\n",
        "\n",
        "avg_neg_sre3 = sentiment_df_sre3['neg'].mean()\n",
        "avg_neu_sre3 = sentiment_df_sre3['neu'].mean()\n",
        "avg_pos_sre3 = sentiment_df_sre3['pos'].mean()\n",
        "\n",
        "# Step 6: Display the average sentiment scores for each column\n",
        "print(\"SRE1 Sentiment Scores:\")\n",
        "print(f\"Average Negative Score: {avg_neg_sre1:.4f}\")\n",
        "print(f\"Average Neutral Score: {avg_neu_sre1:.4f}\")\n",
        "print(f\"Average Positive Score: {avg_pos_sre1:.4f}\\n\")\n",
        "\n",
        "print(\"SRE2 Sentiment Scores:\")\n",
        "print(f\"Average Negative Score: {avg_neg_sre2:.4f}\")\n",
        "print(f\"Average Neutral Score: {avg_neu_sre2:.4f}\")\n",
        "print(f\"Average Positive Score: {avg_pos_sre2:.4f}\\n\")\n",
        "\n",
        "print(\"SRE3 Sentiment Scores:\")\n",
        "print(f\"Average Negative Score: {avg_neg_sre3:.4f}\")\n",
        "print(f\"Average Neutral Score: {avg_neu_sre3:.4f}\")\n",
        "print(f\"Average Positive Score: {avg_pos_sre3:.4f}\")\n"
      ],
      "metadata": {
        "id": "MBZ2RC8CMkcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gGjkau0vcKxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Average sentiment score plot"
      ],
      "metadata": {
        "id": "0W3_uDhBM3fm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Data for the bar chart\n",
        "avg_scores = {\n",
        "    'SRE1 Negative': avg_neg_sre1,\n",
        "    'SRE1 Neutral': avg_neu_sre1,\n",
        "    'SRE1 Positive': avg_pos_sre1,\n",
        "    'SRE2 Negative': avg_neg_sre2,\n",
        "    'SRE2 Neutral': avg_neu_sre2,\n",
        "    'SRE2 Positive': avg_pos_sre2,\n",
        "    'SRE3 Negative': avg_neg_sre3,\n",
        "    'SRE3 Neutral': avg_neu_sre3,\n",
        "    'SRE3 Positive': avg_pos_sre3\n",
        "}\n",
        "\n",
        "# Step 1: Create a DataFrame from the scores\n",
        "avg_scores_df = pd.DataFrame(list(avg_scores.items()), columns=['Sentiment', 'Average Score'])\n",
        "\n",
        "# Step 2: Plot the bar chart\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.barplot(x='Sentiment', y='Average Score', data=avg_scores_df, palette='Blues_d')\n",
        "\n",
        "# Step 3: Customize the plot\n",
        "plt.title('Average Sentiment Scores for SRE1, SRE2, and SRE3')\n",
        "plt.xlabel('Sentiment Category')\n",
        "plt.ylabel('Average Score')\n",
        "plt.xticks(rotation=45)  # Rotate x labels for better readability\n",
        "plt.ylim(0, 1)  # Since sentiment scores range between 0 and 1\n",
        "\n",
        "# Step 4: Display the plot\n",
        "plt.tight_layout()  # Ensure everything fits within the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jN80JjsUM2zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Negative sentiment Keyword analysis"
      ],
      "metadata": {
        "id": "UQRTjOMIMRNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Download the VADER lexicon\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Initialize the VADER sentiment analyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Step 1: Define a function to get sentiment scores\n",
        "def get_sentiment_scores(text):\n",
        "    return sia.polarity_scores(text)\n",
        "\n",
        "# Step 2: Apply the sentiment function to the 'SRE1', 'SRE2', and 'SRE3' columns\n",
        "# Assuming 'SRE1', 'SRE2', and 'SRE3' contain your text data\n",
        "sentiment_scores_sre1 = data['SRE1'].apply(get_sentiment_scores)\n",
        "sentiment_scores_sre2 = data['SRE2'].apply(get_sentiment_scores)\n",
        "sentiment_scores_sre3 = data['SRE3'].apply(get_sentiment_scores)\n",
        "\n",
        "# Step 3: Convert the sentiment scores into DataFrames\n",
        "sentiment_df_sre1 = pd.DataFrame(sentiment_scores_sre1.tolist())\n",
        "sentiment_df_sre2 = pd.DataFrame(sentiment_scores_sre2.tolist())\n",
        "sentiment_df_sre3 = pd.DataFrame(sentiment_scores_sre3.tolist())\n",
        "\n",
        "# Step 4: Rename the columns for better distinction\n",
        "sentiment_df_sre1.columns = ['neg_sre1', 'neu_sre1', 'pos_sre1', 'compound_sre1']\n",
        "sentiment_df_sre2.columns = ['neg_sre2', 'neu_sre2', 'pos_sre2', 'compound_sre2']\n",
        "sentiment_df_sre3.columns = ['neg_sre3', 'neu_sre3', 'pos_sre3', 'compound_sre3']\n",
        "\n",
        "# Step 5: Combine the sentiment scores DataFrames with the original data\n",
        "data = pd.concat([data, sentiment_df_sre1, sentiment_df_sre2, sentiment_df_sre3], axis=1)\n",
        "\n",
        "# Step 6: Check if 'neg_sre1', 'neg_sre2', or 'neg_sre3' columns exist and filter negative texts\n",
        "# Now 'data' should have 'neg', 'neu', 'pos', 'compound' columns for each SRE column\n",
        "negative_texts_sre1 = data[data['neg_sre1'] > 0.2]['SRE1']  # Adjust threshold as needed for 'SRE1'\n",
        "negative_texts_sre2 = data[data['neg_sre2'] > 0.2]['SRE2']  # Adjust threshold as needed for 'SRE2'\n",
        "negative_texts_sre3 = data[data['neg_sre3'] > 0.2]['SRE3']  # Adjust threshold as needed for 'SRE3'\n",
        "\n",
        "# Step 7: Display the first few rows to check the result\n",
        "# print(data.head())\n"
      ],
      "metadata": {
        "id": "ixZaTxYqMUol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Keyword analysis with Negative sentiment (To check) - skip this"
      ],
      "metadata": {
        "id": "fTxhfd_5PTS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the negative word counts\n",
        "# print(negative_word_counts)\n",
        "\n",
        "# Check if negative texts exist\n",
        "# print(data[data['neg'] > 0.1]['SRE1'])\n",
        "\n",
        "print(data['neg'].describe())\n"
      ],
      "metadata": {
        "id": "-g0O8ty-PnMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the data types in the 'SRE1' column\n",
        "print(data['SRE1'].apply(type).value_counts())\n"
      ],
      "metadata": {
        "id": "AgMsIbPrRb8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the tokenized words in the negative texts\n",
        "for text in data[data['neg'] > 0.02]['SRE1']: # adjust threshold\n",
        "    tokens = tokenize_and_clean(text)\n",
        "    print(f\"Text: {text}\")\n",
        "    print(f\"Tokens: {tokens}\")\n"
      ],
      "metadata": {
        "id": "UlzlrterQweL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Most common negative words"
      ],
      "metadata": {
        "id": "EBHL_Xs1etIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "# Define a function to tokenize and clean text (removing stopwords)\n",
        "def tokenize_and_clean(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [word for word in tokens if word.isalnum() and word.lower() not in stop_words]\n",
        "\n",
        "# Tokenize and count words in negative texts\n",
        "negative_words = []\n",
        "for text in data[data['neg'] > 0.02]['SRE1']:  # Adjust the threshold if needed\n",
        "    negative_words.extend(tokenize_and_clean(text))\n",
        "\n",
        "# Count frequency of words in negative texts\n",
        "negative_word_counts = Counter(negative_words)\n",
        "\n",
        "# Display the most common negative words\n",
        "print(negative_word_counts.most_common(20))\n",
        "\n",
        "# Plot the most common negative words\n",
        "common_words = negative_word_counts.most_common(10)\n",
        "words, counts = zip(*common_words)\n",
        "plt.bar(words, counts)\n",
        "plt.title('Most Common Negative Words')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rf2jcCojPXVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network analysis (This plot is too big)"
      ],
      "metadata": {
        "id": "oMA_3Uc0Symi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import combinations\n",
        "from collections import defaultdict\n",
        "\n",
        "# Download necessary nltk data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Step 1: Tokenization and Cleaning Function\n",
        "def tokenize_and_clean(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize\n",
        "    return [word for word in tokens if word.isalnum() and word not in stop_words]  # Remove stopwords and non-alphanumeric tokens\n",
        "\n",
        "# Step 2: Filter rows where 'SRE1' is valid text and tokenize\n",
        "data = data[data['SRE1'].apply(lambda x: isinstance(x, str))]  # Ensure only strings\n",
        "tokenized_texts = data['SRE1'].apply(tokenize_and_clean)\n",
        "\n",
        "# Step 3: Create a co-occurrence dictionary (word pairs as keys, counts as values)\n",
        "co_occurrence = defaultdict(int)\n",
        "\n",
        "for tokens in tokenized_texts:\n",
        "    for pair in combinations(tokens, 2):  # Create pairs of words within each text\n",
        "        co_occurrence[pair] += 1\n",
        "\n",
        "# Step 4: Build the network graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add edges based on co-occurrence frequency\n",
        "for (word1, word2), weight in co_occurrence.items():\n",
        "    if weight > 1:  # Filter by weight (frequency of co-occurrence)\n",
        "        G.add_edge(word1, word2, weight=weight)\n",
        "\n",
        "# Step 5: Visualize the Network Graph\n",
        "plt.figure(figsize=(12, 12))\n",
        "pos = nx.spring_layout(G, k=0.15, iterations=20)  # Generate layout for nodes\n",
        "nx.draw_networkx_nodes(G, pos, node_size=50, node_color='skyblue')\n",
        "nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.7)\n",
        "nx.draw_networkx_labels(G, pos, font_size=10, font_color='black')\n",
        "\n",
        "plt.title(\"Keyword Co-occurrence Network\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Oty_P2f4TLHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "# Step 5: Visualize the Network Graph with improved settings\n",
        "plt.figure(figsize=(15, 15))\n",
        "\n",
        "# Generate layout for nodes\n",
        "pos = nx.spring_layout(G, k=0.15, iterations=20)\n",
        "\n",
        "# Draw nodes with larger size\n",
        "nx.draw_networkx_nodes(G, pos, node_size=500, node_color='skyblue', alpha=0.8)\n",
        "\n",
        "# Draw edges with transparency and reduced weight\n",
        "nx.draw_networkx_edges(G, pos, width=0.5, alpha=0.3, edge_color='black')\n",
        "\n",
        "# Draw labels with larger font size\n",
        "nx.draw_networkx_labels(G, pos, font_size=10, font_color='darkblue')\n",
        "\n",
        "plt.title(\"Keyword Co-occurrence Network\", size=15)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3SZ7wUWoTmB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reduce frequency filtering words"
      ],
      "metadata": {
        "id": "AWDGmXhVUHva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "\n",
        "# Assuming you have a co-occurrence dictionary with word pairs and their frequencies\n",
        "# Example: co_occurrence = {('word1', 'word2'): 5, ('word3', 'word4'): 3, ...}\n",
        "\n",
        "# Step 1: Filter the co-occurrence pairs\n",
        "co_occurrence_filtered = {pair: count for pair, count in co_occurrence.items() if count > 30}\n",
        "\n",
        "# Step 2: Create a NetworkX graph object\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add edges to the graph\n",
        "for (word1, word2), freq in co_occurrence_filtered.items():\n",
        "    G.add_edge(word1, word2, weight=freq)\n",
        "\n",
        "# Step 3: Plot the network\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "# Use spring layout for a better visual appearance\n",
        "pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
        "\n",
        "# Draw nodes with size proportional to their degree (number of connections)\n",
        "nx.draw_networkx_nodes(G, pos, node_size=[v * 100 for v in dict(G.degree()).values()], node_color='skyblue', alpha=0.7)\n",
        "\n",
        "# Draw edges with width proportional to the frequency of co-occurrence\n",
        "nx.draw_networkx_edges(G, pos, width=[G[u][v]['weight'] * 0.1 for u, v in G.edges()], edge_color='gray', alpha=0.5)\n",
        "\n",
        "# Draw labels\n",
        "nx.draw_networkx_labels(G, pos, font_size=15, font_color='black')\n",
        "\n",
        "# Set plot title\n",
        "plt.title(\"Filtered Co-occurrence Network (Q1)\", size=20)\n",
        "plt.axis(\"off\")  # Hide axis\n",
        "\n",
        "plt.savefig(\"Network01-Q1.png\")  # Save the plot as an image\n",
        "plt.show()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DhzqDzhsUrL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2"
      ],
      "metadata": {
        "id": "Z2_VX4f8fVrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from itertools import combinations\n",
        "\n",
        "# Step 1: Preprocess the text data from SRE2\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Tokenization function to preprocess the text\n",
        "def tokenize_and_clean(text):\n",
        "    # Ensure the text is a string, convert non-string (e.g., float) to empty string\n",
        "    if isinstance(text, float):\n",
        "        text = \"\"\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    return [token for token in tokens if token.isalpha() and token not in stop_words]\n",
        "\n",
        "# Step 2: Generate the co-occurrence dictionary for 'SRE2'\n",
        "def generate_co_occurrence(text_column):\n",
        "    co_occurrence_dict = defaultdict(int)\n",
        "\n",
        "    # Loop through each document and tokenize it\n",
        "    for text in text_column:\n",
        "        tokens = tokenize_and_clean(text)\n",
        "\n",
        "        # Create co-occurrence pairs from tokens in each document\n",
        "        for pair in combinations(set(tokens), 2):\n",
        "            co_occurrence_dict[tuple(sorted(pair))] += 1\n",
        "\n",
        "    return co_occurrence_dict\n",
        "\n",
        "# Assuming your DataFrame is named 'data' and 'SRE2' contains the text data\n",
        "# Convert NaN values to empty strings\n",
        "data['SRE2'] = data['SRE2'].fillna('')\n",
        "\n",
        "co_occurrence_sre2 = generate_co_occurrence(data['SRE2'])\n",
        "\n",
        "# Step 3: Filter the co-occurrence pairs for 'SRE2' with frequency greater than a threshold (e.g., 30)\n",
        "frequency_threshold = 5  # Adjust the threshold as needed\n",
        "co_occurrence_filtered_sre2 = {pair: count for pair, count in co_occurrence_sre2.items() if count > frequency_threshold}\n",
        "\n",
        "# Step 4: Create a NetworkX graph object for 'SRE2'\n",
        "G_sre2 = nx.Graph()\n",
        "\n",
        "# Add edges to the graph for 'SRE2'\n",
        "for (word1, word2), freq in co_occurrence_filtered_sre2.items():\n",
        "    G_sre2.add_edge(word1, word2, weight=freq)\n",
        "\n",
        "# Step 5: Plot the network for 'SRE2'\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "# Use spring layout for a better visual appearance\n",
        "pos_sre2 = nx.spring_layout(G_sre2, k=0.5, iterations=50)\n",
        "\n",
        "# Draw nodes with size proportional to their degree (number of connections)\n",
        "nx.draw_networkx_nodes(G_sre2, pos_sre2, node_size=[v * 100 for v in dict(G_sre2.degree()).values()], node_color='skyblue', alpha=0.7)\n",
        "\n",
        "# Draw edges with width proportional to the frequency of co-occurrence\n",
        "nx.draw_networkx_edges(G_sre2, pos_sre2, width=[G_sre2[u][v]['weight'] * 0.1 for u, v in G_sre2.edges()], edge_color='gray', alpha=0.5)\n",
        "\n",
        "# Draw labels\n",
        "nx.draw_networkx_labels(G_sre2, pos_sre2, font_size=15, font_color='black')\n",
        "\n",
        "# Set plot title\n",
        "plt.title(\"Filtered Co-occurrence Network (Q2)\", size=20)\n",
        "plt.axis(\"off\")  # Hide axis\n",
        "\n",
        "plt.savefig(\"Network01-SRE2.png\")  # Save the plot as an image\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QIW-cONTfwCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SRE3"
      ],
      "metadata": {
        "id": "49pkyxD9he_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from itertools import combinations\n",
        "\n",
        "# Step 1: Preprocess the text data from SRE3\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Tokenization function to preprocess the text\n",
        "def tokenize_and_clean(text):\n",
        "    # Ensure the text is a string, convert non-string (e.g., float) to empty string\n",
        "    if isinstance(text, float):\n",
        "        text = \"\"\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    return [token for token in tokens if token.isalpha() and token not in stop_words]\n",
        "\n",
        "# Step 2: Generate the co-occurrence dictionary for 'SRE3'\n",
        "def generate_co_occurrence(text_column):\n",
        "    co_occurrence_dict = defaultdict(int)\n",
        "\n",
        "    # Loop through each document and tokenize it\n",
        "    for text in text_column:\n",
        "        tokens = tokenize_and_clean(text)\n",
        "\n",
        "        # Create co-occurrence pairs from tokens in each document\n",
        "        for pair in combinations(set(tokens), 2):\n",
        "            co_occurrence_dict[tuple(sorted(pair))] += 1\n",
        "\n",
        "    return co_occurrence_dict\n",
        "\n",
        "# Assuming your DataFrame is named 'data' and 'SRE3' contains the text data\n",
        "# Convert NaN values to empty strings\n",
        "data['SRE3'] = data['SRE3'].fillna('')\n",
        "\n",
        "co_occurrence_sre3 = generate_co_occurrence(data['SRE3'])\n",
        "\n",
        "# Step 3: Filter the co-occurrence pairs for 'SRE3' with frequency greater than a threshold (e.g., 5)\n",
        "frequency_threshold = 5  # Adjust the threshold as needed\n",
        "co_occurrence_filtered_sre3 = {pair: count for pair, count in co_occurrence_sre3.items() if count > frequency_threshold}\n",
        "\n",
        "# Step 4: Create a NetworkX graph object for 'SRE3'\n",
        "G_sre3 = nx.Graph()\n",
        "\n",
        "# Add edges to the graph for 'SRE3'\n",
        "for (word1, word2), freq in co_occurrence_filtered_sre3.items():\n",
        "    G_sre3.add_edge(word1, word2, weight=freq)\n",
        "\n",
        "# Step 5: Plot the network for 'SRE3'\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "# Use spring layout for a better visual appearance\n",
        "pos_sre3 = nx.spring_layout(G_sre3, k=0.5, iterations=50)\n",
        "\n",
        "# Draw nodes with size proportional to their degree (number of connections)\n",
        "nx.draw_networkx_nodes(G_sre3, pos_sre3, node_size=[v * 100 for v in dict(G_sre3.degree()).values()], node_color='skyblue', alpha=0.7)\n",
        "\n",
        "# Draw edges with width proportional to the frequency of co-occurrence\n",
        "nx.draw_networkx_edges(G_sre3, pos_sre3, width=[G_sre3[u][v]['weight'] * 0.1 for u, v in G_sre3.edges()], edge_color='gray', alpha=0.5)\n",
        "\n",
        "# Draw labels\n",
        "nx.draw_networkx_labels(G_sre3, pos_sre3, font_size=15, font_color='black')\n",
        "\n",
        "# Set plot title\n",
        "plt.title(\"Filtered Co-occurrence Network (SRE3)\", size=20)\n",
        "plt.axis(\"off\")  # Hide axis\n",
        "\n",
        "plt.savefig(\"Network01-SRE3.png\")  # Save the plot as an image\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "RVaCJXZfhg3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting filtering frequency number objectively"
      ],
      "metadata": {
        "id": "aWVlBWl3W7-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenize and clean text data (assuming 'SRE1' is your column name)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Combine all text into one\n",
        "all_text = \" \".join(data['SRE1'].dropna().tolist())\n",
        "\n",
        "# Tokenize words\n",
        "tokens = word_tokenize(all_text)\n",
        "\n",
        "# Remove stopwords and non-alphabetic tokens\n",
        "cleaned_tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
        "\n",
        "# Calculate word frequencies\n",
        "word_counts = Counter(cleaned_tokens)\n",
        "\n",
        "# Set a frequency threshold for filtering\n",
        "frequency_threshold = 8\n",
        "\n",
        "# Filter words that occur more than the threshold\n",
        "filtered_words = {word: count for word, count in word_counts.items() if count > frequency_threshold}\n",
        "\n",
        "# Create co-occurrence network\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add edges based on co-occurrence of words in the same text\n",
        "for text in data['SRE1'].dropna():\n",
        "    words = word_tokenize(text)\n",
        "    words = [word.lower() for word in words if word.isalpha() and word.lower() in filtered_words]\n",
        "\n",
        "    # Add edges between all pairs of co-occurring words\n",
        "    for i, word in enumerate(words):\n",
        "        for j in range(i+1, len(words)):\n",
        "            G.add_edge(word, words[j])\n",
        "\n",
        "# Draw network with filtered words\n",
        "plt.figure(figsize=(10, 8))\n",
        "pos = nx.spring_layout(G, k=0.5)\n",
        "nx.draw(G, pos, with_labels=True, node_size=7000, node_color='lightblue', font_size=10, edge_color='gray')\n",
        "plt.title(\"Filtered Co-occurrence Network (Q1)\")\n",
        "plt.savefig(\"Network-keywords-Q1.png\")  # Save the plot as an image\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yyBXrKlKW_62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SRE2"
      ],
      "metadata": {
        "id": "JX540-igh9Iw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenize and clean text data (now analyzing 'SRE2' column)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Combine all text into one from the 'SRE2' column\n",
        "all_text = \" \".join(data['SRE2'].dropna().tolist())\n",
        "\n",
        "# Tokenize words\n",
        "tokens = word_tokenize(all_text)\n",
        "\n",
        "# Remove stopwords and non-alphabetic tokens\n",
        "cleaned_tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
        "\n",
        "# Calculate word frequencies\n",
        "word_counts = Counter(cleaned_tokens)\n",
        "\n",
        "# Set a frequency threshold for filtering\n",
        "frequency_threshold = 7\n",
        "\n",
        "# Filter words that occur more than the threshold\n",
        "filtered_words = {word: count for word, count in word_counts.items() if count > frequency_threshold}\n",
        "\n",
        "# Create co-occurrence network\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add edges based on co-occurrence of words in the same text in the 'SRE2' column\n",
        "for text in data['SRE2'].dropna():\n",
        "    words = word_tokenize(text)\n",
        "    words = [word.lower() for word in words if word.isalpha() and word.lower() in filtered_words]\n",
        "\n",
        "    # Add edges between all pairs of co-occurring words\n",
        "    for i, word in enumerate(words):\n",
        "        for j in range(i+1, len(words)):\n",
        "            G.add_edge(word, words[j])\n",
        "\n",
        "# Draw network with filtered words\n",
        "plt.figure(figsize=(10, 8))\n",
        "pos = nx.spring_layout(G, k=0.5)\n",
        "nx.draw(G, pos, with_labels=True, node_size=7000, node_color='lightblue', font_size=10, edge_color='gray')\n",
        "plt.title(\"Filtered Co-occurrence Network (Q2)\")\n",
        "plt.savefig(\"Network-keywords-Q2.png\")  # Save the plot as an image\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "okFInQrFh-Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3"
      ],
      "metadata": {
        "id": "91AKmeXciyiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenize and clean text data (now analyzing 'SRE2' column)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Combine all text into one from the 'SRE2' column\n",
        "all_text = \" \".join(data['SRE3'].dropna().tolist())\n",
        "\n",
        "# Tokenize words\n",
        "tokens = word_tokenize(all_text)\n",
        "\n",
        "# Remove stopwords and non-alphabetic tokens\n",
        "cleaned_tokens = [word.lower() for word in tokens if word.isalpha() and word.lower() not in stop_words]\n",
        "\n",
        "# Calculate word frequencies\n",
        "word_counts = Counter(cleaned_tokens)\n",
        "\n",
        "# Set a frequency threshold for filtering\n",
        "frequency_threshold = 7\n",
        "\n",
        "# Filter words that occur more than the threshold\n",
        "filtered_words = {word: count for word, count in word_counts.items() if count > frequency_threshold}\n",
        "\n",
        "# Create co-occurrence network\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add edges based on co-occurrence of words in the same text in the 'SRE2' column\n",
        "for text in data['SRE3'].dropna():\n",
        "    words = word_tokenize(text)\n",
        "    words = [word.lower() for word in words if word.isalpha() and word.lower() in filtered_words]\n",
        "\n",
        "    # Add edges between all pairs of co-occurring words\n",
        "    for i, word in enumerate(words):\n",
        "        for j in range(i+1, len(words)):\n",
        "            G.add_edge(word, words[j])\n",
        "\n",
        "# Draw network with filtered words\n",
        "plt.figure(figsize=(10, 8))\n",
        "pos = nx.spring_layout(G, k=0.5)\n",
        "nx.draw(G, pos, with_labels=True, node_size=7000, node_color='lightblue', font_size=10, edge_color='gray')\n",
        "plt.title(\"Filtered Co-occurrence Network (Q3)\")\n",
        "plt.savefig(\"Network-keywords-Q3.png\")  # Save the plot as an image\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ij_HZjW6i0Rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combine data and save: SRE1, SRE2, SRE3"
      ],
      "metadata": {
        "id": "D3FNujIHjghB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming 'data' is your DataFrame and 'SRE1', 'SRE2', 'SRE3' are your columns\n",
        "\n",
        "# Step 1: Combine the 'SRE1', 'SRE2', and 'SRE3' into a new column 'SRE-total'\n",
        "data['SRE-total'] = data[['SRE1', 'SRE2', 'SRE3']].apply(lambda x: ' '.join(x.dropna()), axis=1)\n",
        "\n",
        "# Step 2: Save the DataFrame to a CSV file with proper encoding (UTF-8)\n",
        "file_path = '/content/SRE-combineddata.csv'  # Adjust the path based on your requirement\n",
        "data.to_csv(file_path, encoding='utf-8-sig', index=False)\n",
        "\n",
        "print(f\"File saved successfully to {file_path}\")\n"
      ],
      "metadata": {
        "id": "RfKlW-Hwjkh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combined data analysis"
      ],
      "metadata": {
        "id": "4lR-99f4mXSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filtered"
      ],
      "metadata": {
        "id": "Ama1kZmvmhpj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RSi9tvwWnGL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from itertools import combinations\n",
        "\n",
        "# Step 1: Preprocess the text data from SRE3\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Tokenization function to preprocess the text\n",
        "def tokenize_and_clean(text):\n",
        "    # Ensure the text is a string, convert non-string (e.g., float) to empty string\n",
        "    if isinstance(text, float):\n",
        "        text = \"\"\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    return [token for token in tokens if token.isalpha() and token not in stop_words]\n",
        "\n",
        "# Step 2: Generate the co-occurrence dictionary for 'SRE3'\n",
        "def generate_co_occurrence(text_column):\n",
        "    co_occurrence_dict = defaultdict(int)\n",
        "\n",
        "    # Loop through each document and tokenize it\n",
        "    for text in text_column:\n",
        "        tokens = tokenize_and_clean(text)\n",
        "\n",
        "        # Create co-occurrence pairs from tokens in each document\n",
        "        for pair in combinations(set(tokens), 2):\n",
        "            co_occurrence_dict[tuple(sorted(pair))] += 1\n",
        "\n",
        "    return co_occurrence_dict\n",
        "\n",
        "# Assuming your DataFrame is named 'data' and 'SRE3' contains the text data\n",
        "# Convert NaN values to empty strings\n",
        "data['SRE-total'] = data['SRE-total'].fillna('')\n",
        "\n",
        "co_occurrence_sre3 = generate_co_occurrence(data['SRE-total'])\n",
        "\n",
        "# Step 3: Filter the co-occurrence pairs for 'SRE3' with frequency greater than a threshold (e.g., 5)\n",
        "frequency_threshold = 10  # Adjust the threshold as needed\n",
        "co_occurrence_filtered_sre3 = {pair: count for pair, count in co_occurrence_sre3.items() if count > frequency_threshold}\n",
        "\n",
        "# Step 4: Create a NetworkX graph object for 'SRE3'\n",
        "G_sre3 = nx.Graph()\n",
        "\n",
        "# Add edges to the graph for 'SRE3'\n",
        "for (word1, word2), freq in co_occurrence_filtered_sre3.items():\n",
        "    G_sre3.add_edge(word1, word2, weight=freq)\n",
        "\n",
        "# Step 5: Plot the network for 'SRE3'\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "# Use spring layout for a better visual appearance\n",
        "pos_sre3 = nx.spring_layout(G_sre3, k=0.5, iterations=50)\n",
        "\n",
        "# Draw nodes with size proportional to their degree (number of connections)\n",
        "nx.draw_networkx_nodes(G_sre3, pos_sre3, node_size=[v * 100 for v in dict(G_sre3.degree()).values()], node_color='skyblue', alpha=0.7)\n",
        "\n",
        "# Draw edges with width proportional to the frequency of co-occurrence\n",
        "nx.draw_networkx_edges(G_sre3, pos_sre3, width=[G_sre3[u][v]['weight'] * 0.1 for u, v in G_sre3.edges()], edge_color='gray', alpha=0.5)\n",
        "\n",
        "# Draw labels\n",
        "nx.draw_networkx_labels(G_sre3, pos_sre3, font_size=15, font_color='black')\n",
        "\n",
        "# Set plot title\n",
        "plt.title(\"Filtered Co-occurrence Network (SRE-total)\", size=20)\n",
        "plt.axis(\"off\")  # Hide axis\n",
        "\n",
        "plt.savefig(\"Network01-SRE-total.png\")  # Save the plot as an image\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qHW6MrAPmZfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "The End"
      ],
      "metadata": {
        "id": "bnsC3dFelcOl"
      }
    }
  ]
}